{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc4f037",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715e91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavy imports\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4316f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers import get_constant_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96fb7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# check if gpus are available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# gpu details: preferred A100 with 256 GBs\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf0e46",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19500281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paths\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # base model\n",
    "output_dir = \"./fashion-model-finetuned\"  # where to save the model\n",
    "\n",
    "# dataset parameters\n",
    "dataset_name = \"lambdalabs/pokemon-blip-captions\"  # replace with your dataset\n",
    "image_column = \"image\"  # column name containing images\n",
    "caption_column = \"text\"  # column name containing captions\n",
    "\n",
    "# training parameters\n",
    "resolution = 512  # image resolution for training\n",
    "train_batch_size = 4  # adjust based on your gpu memory\n",
    "num_train_epochs = 10  # number of training epochs\n",
    "max_train_steps = None  # if set, overrides num_train_epochs\n",
    "gradient_accumulation_steps = 1  # for effective larger batch size\n",
    "learning_rate = 1e-5  # training learning rate\n",
    "lr_scheduler = \"constant\"  # learning rate scheduler type\n",
    "lr_warmup_steps = 0  # warmup steps for learning rate\n",
    "adam_beta1 = 0.9  # adam optimizer beta1\n",
    "adam_beta2 = 0.999  # adam optimizer beta2\n",
    "adam_weight_decay = 1e-2  # weight decay for regularization\n",
    "adam_epsilon = 1e-08  # small epsilon for numerical stability\n",
    "max_grad_norm = 1.0  # gradient clipping norm\n",
    "checkpointing_steps = 500  # save checkpoint every n steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7e489",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f922ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eqp6pg/.conda/envs/my-torch/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      "- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet parameters to be trained: 859535364\n"
     ]
    }
   ],
   "source": [
    "# noise scheduler for the diffusion process\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "# tokenizer for processing text prompts\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    subfolder=\"tokenizer\"\n",
    ")\n",
    "\n",
    "# text encoder to convert text to embeddings\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "# vae model for encoding images to latent space and vice versa\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# unet model - the part we'll be fine-tuning\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False, \n",
    "    ignore_mismatched_sizes=True,\n",
    "    in_channels=9,  # modified for mask and masked image\n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# move models to device (gpu if available)\n",
    "text_encoder = text_encoder.to(device)\n",
    "vae = vae.to(device)\n",
    "unet = unet.to(device)\n",
    "\n",
    "# freeze vae and text encoder - we only want to train the UNet\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "# print model parameters information\n",
    "print(f\"UNet parameters to be trained: {sum(p.numel() for p in unet.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca2c49",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "The dataset needs to include images, text prompts, and masks:\n",
    "- Images: The fashion items/outfits\n",
    "- Text: Descriptions of the desired edits\n",
    "- Masks: Binary masks identifying regions to edit\n",
    "\n",
    "**Dataset Format Requirements**\n",
    "\n",
    "Dataset Type: The code expects a HuggingFace dataset that can be loaded with load_dataset()\n",
    "\n",
    "**Required Columns**\n",
    "\n",
    "- image: Contains image data\n",
    "- text: Contains the caption/description for each image\n",
    "- mask: Contains segmentation masks for each image\n",
    "\n",
    "**Image Format**\n",
    "\n",
    "- The images should be convertible to RGB format with .convert(\"RGB\")\n",
    "- Typically these would be PIL Image objects\n",
    "- The image dimensions can vary as they get resized during preprocessing\n",
    "\n",
    "**Mask Format**\n",
    "\n",
    "- Should be compatible with torchvision transforms\n",
    "- Typically a grayscale image where pixel values indicate which area to mask\n",
    "- Will be converted to a binary mask (0/1 values)\n",
    "\n",
    "**Text Format**\n",
    "\n",
    "- A string containing the caption\n",
    "- A list/array of strings (from which one will be randomly selected)\n",
    "\n",
    "**Example**\n",
    "\n",
    "```\n",
    "{\n",
    "    \"image\": <PIL.Image object>,       # RGB image\n",
    "    \"mask\": <PIL.Image object>,        # Grayscale/binary mask\n",
    "    \"text\": \"Description of the image\"  # or [\"caption1\", \"caption2\", ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c68bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transforms for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        (resolution, resolution // 2), \n",
    "        interpolation=transforms.InterpolationMode.BILINEAR\n",
    "    ),\n",
    "    transforms.CenterCrop(resolution),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# mask transforms\n",
    "train_mask_transforms = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        (resolution, resolution // 2), \n",
    "        interpolation=transforms.InterpolationMode.NEAREST\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]  # convert images to RGB format\n",
    "    masks = [mask for mask in examples['mask']]  # extract masks (assumes dataset has a 'mask' column)\n",
    "    \n",
    "    # apply transforms\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"pixel_mask_values\"] = [train_mask_transforms(mask) for mask in masks]\n",
    "    \n",
    "    # create masked pixel values by applying mask to image\n",
    "    examples[\"masked_pixel_values\"] = [\n",
    "        torch.masked_fill(\n",
    "            examples[\"pixel_values\"][i], \n",
    "            examples[\"pixel_mask_values\"][i].bool(), \n",
    "            0\n",
    "        ) for i in range(len(images))\n",
    "    ]\n",
    "    \n",
    "    # tokenize the captions\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            captions.append(random.choice(caption))\n",
    "        else:\n",
    "            raise ValueError(\"Caption column should contain strings or lists of strings.\")\n",
    "            \n",
    "    inputs = tokenizer(\n",
    "        captions, \n",
    "        max_length=tokenizer.model_max_length, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    examples[\"input_ids\"] = inputs.input_ids\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# apply preprocessing to the dataset\n",
    "train_dataset = dataset[\"train\"].with_transform(preprocess_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function for the dataloader\n",
    "def collate_fn(examples):\n",
    "    # stack the tensors\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()    \n",
    "    pixel_mask_values = torch.stack([example[\"pixel_mask_values\"] for example in examples])\n",
    "    pixel_mask_values = pixel_mask_values.to(memory_format=torch.contiguous_format)\n",
    "    masked_pixel_values = torch.stack([example[\"masked_pixel_values\"] for example in examples])\n",
    "    masked_pixel_values = masked_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    # stack the tokenized text\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values, \n",
    "        \"pixel_mask_values\": pixel_mask_values,\n",
    "        \"masked_pixel_values\": masked_pixel_values, \n",
    "        \"input_ids\": input_ids\n",
    "    }\n",
    "\n",
    "# create DataLoader for training\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_batch_size,\n",
    "    num_workers=2,  # adjust based on your CPU (can be found out using os.cpu_count())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be95fa",
   "metadata": {},
   "source": [
    "# Optimizer and Scheduler\n",
    "\n",
    "Using the default \"constant\" scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c84cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer for the unet model\n",
    "optimizer = torch.optim.AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay,\n",
    "    eps=adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of steps for training\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64551a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create learning rate scheduler\n",
    "lr_scheduler = get_constant_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps, \n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training info\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num epochs = {num_train_epochs}\")\n",
    "print(f\"  Batch size per device = {train_batch_size}\")\n",
    "print(f\"  Gradient accumulation steps = {gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba19bcc",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Didn't understand the gradient accumulation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_train_epochs}\")\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # convert images to latent space using vae\n",
    "        latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "        # encode the masked images\n",
    "        masked_latents = vae.encode(batch[\"masked_pixel_values\"]).latent_dist.sample()\n",
    "        masked_latents = masked_latents * vae.config.scaling_factor\n",
    "        \n",
    "        # resize mask to match latent resolution\n",
    "        mask = F.interpolate(\n",
    "            batch[\"pixel_mask_values\"],\n",
    "            size=(resolution // 8, resolution // 16),  # Match aspect ratio\n",
    "            mode='nearest'\n",
    "        )\n",
    "        \n",
    "        # sample noise for diffusion\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # sample random timesteps for diffusion process\n",
    "        bsz = latents.shape[0]\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device\n",
    "        ).long()\n",
    "        \n",
    "        # add noise to latents according to noise schedule (forward diffusion)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # concatenate the inputs for the unet\n",
    "        latent_model_input = torch.cat([noisy_latents, mask, masked_latents], dim=1)\n",
    "        \n",
    "        # get text embeddings for conditioning\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "        \n",
    "        # determine the target for loss calculation based on prediction type\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise  # predict the noise that was added\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "        \n",
    "        # forward pass through unet to predict noise\n",
    "        model_pred = unet(\n",
    "            latent_model_input, \n",
    "            timesteps, \n",
    "            encoder_hidden_states\n",
    "        ).sample\n",
    "        \n",
    "        # calculate loss between prediction and target\n",
    "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        \n",
    "        # calculate average loss for logging\n",
    "        avg_loss = loss.item()\n",
    "        train_loss += avg_loss / gradient_accumulation_steps\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # increment global step and log progress\n",
    "            global_step += 1\n",
    "            \n",
    "            # log the loss\n",
    "            if global_step % 10 == 0:\n",
    "                print(f\"Step {global_step}: Loss: {train_loss:.5f}\")\n",
    "                train_loss = 0.0\n",
    "            \n",
    "            # save checkpoint\n",
    "            if global_step % checkpointing_steps == 0:\n",
    "                print(f\"Saving checkpoint at step {global_step}\")\n",
    "                save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                \n",
    "                # save the unet model\n",
    "                unet.save_pretrained(os.path.join(save_path, \"unet\"))\n",
    "                \n",
    "                print(f\"Saved checkpoint to {save_path}\")\n",
    "            \n",
    "        # check if we've reached max steps\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "    \n",
    "    # end of epoch\n",
    "    print(f\"Epoch {epoch+1} finished\")\n",
    "    \n",
    "    # check if we've reached max steps\n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "# end of training - save the final model\n",
    "print(\"Training complete!\")\n",
    "print(f\"Saving final model to {output_dir}\")\n",
    "\n",
    "# save the final unet\n",
    "unet.save_pretrained(os.path.join(output_dir, \"unet\"))\n",
    "\n",
    "# create and save the full pipeline for inference\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    text_encoder=text_encoder,\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    ")\n",
    "\n",
    "# save the pipeline\n",
    "pipeline.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
