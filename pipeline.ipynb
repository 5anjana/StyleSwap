{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes the images and XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Created Modules \n",
    "These modules are in the textfit github. In order for this code to fully function, we would need to include the classes and functions in this notebook, or keep them as modules in our repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import create_model\n",
    "#from utils.options import dict_to_nonedict, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images\n",
    "\n",
    "This code loads an image and downsamples it by a factor of 2 (to reduce computational cost WE COULD DO MORE?), resizes it, converts the image into a a numpy array, and returns float32 values in in the numpy array.  \n",
    "This prepares the images to be loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image\n",
    "def load_image(image_path):\n",
    "    #set downsample factor\n",
    "    downsample_factor = 2\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        #open image\n",
    "        image = Image.open(f)\n",
    "        width, height = image.size\n",
    "        #donwsample image\n",
    "        width = width // downsample_factor\n",
    "        height = height // downsample_factor\n",
    "        #resize image\n",
    "        image = image.resize(size=(width, height), resample=Image.NEAREST)\n",
    "        #transpose array\n",
    "        image = np.array(image).transpose(2, 0, 1)\n",
    "    #return array\n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents of other files \n",
    "parse function from utils.options  \n",
    "options from yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse from utils.options, needed to read the options\n",
    "\n",
    "def parse(options_dict, is_train=True):\n",
    "    \"\"\"Parse options from a dictionary instead of a YAML file.\n",
    "\n",
    "    Args:\n",
    "        options_dict (dict): Dictionary containing model options.\n",
    "        is_train (bool): Indicates whether in training mode. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed options.\n",
    "    \"\"\"\n",
    "    # create copy of input dictionary to avoid modifying the original\n",
    "    opt = options_dict.copy()\n",
    "\n",
    "    #get gpu list\n",
    "    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n",
    "    if opt.get('set_CUDA_VISIBLE_DEVICES', None):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n",
    "        print('export CUDA_VISIBLE_DEVICES=' + gpu_list, flush=True)\n",
    "    else:\n",
    "        print('gpu_list: ', gpu_list, flush=True)\n",
    "\n",
    "    opt['is_train'] = is_train\n",
    "\n",
    "    #save optional paths (only if user wants to save logs & models)\n",
    "    opt['path'] = {}\n",
    "    opt['path']['root'] = os.getcwd() #set root as current working directory\n",
    "\n",
    "    if is_train:\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['models'] = opt.get('models_path', os.path.join(opt['path']['root'], 'models'))\n",
    "        opt['path']['logs'] = opt.get('logs_path', os.path.join(opt['path']['root'], 'logs'))\n",
    "        opt['path']['visualization'] = opt.get('visualization_path', os.path.join(opt['path']['root'], 'visualization'))\n",
    "        \n",
    "        \n",
    "        # change some options for debug mode\n",
    "        #debug enabled = True\n",
    "        if opt.get('debug', False):\n",
    "            opt['val_freq'] = 1\n",
    "            opt['print_freq'] = 1\n",
    "            opt['save_checkpoint_freq'] = 1\n",
    "\n",
    "    #for test mode    \n",
    "    else:  # test\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['results'] = opt.get('results_path', os.path.join(opt['path']['root'], 'results'))\n",
    "        opt['path']['log'] = opt.get('log_path', os.path.join(opt['path']['root'], 'test_logs'))\n",
    "        opt['path']['visualization'] = opt.get('vis_path', os.path.join(opt['path']['root'], 'test_visualizations'))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of yaml file converted to dictionary?\n",
    "options_dict = {\n",
    "    'name': 'region_gen',\n",
    "    'use_tb_logger': True,\n",
    "    'debug_path': False,\n",
    "    'set_CUDA_VISIBLE_DEVICES': True,\n",
    "    'gpu_ids': [0],\n",
    "\n",
    "    # dataset configs\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    'mask_dir': '/path/to/DFMM-Spotlight/mask',\n",
    "    'train_img_dir': '/path/to/DFMM-Spotlight/train_images',\n",
    "    'test_img_dir': '/path/to/DFMM-Spotlight/test_images',\n",
    "    'train_ann_file': '/path/to/DFMM-Spotlight/mask_ann/train_ann_file.jsonl',\n",
    "    'test_ann_file': '/path/to/DFMM-Spotlight/mask_ann/test_ann_file.jsonl',\n",
    "    'downsample_factor': 2,\n",
    "\n",
    "    # model configs\n",
    "    'model_type': 'ERLM',\n",
    "    'text_embedding_dim': 512,\n",
    "    'encoder_in_channels': 3,\n",
    "    'fc_in_channels': 64,\n",
    "    'fc_in_index': 4,\n",
    "    'fc_channels': 64,\n",
    "    'fc_num_convs': 1,\n",
    "    'fc_concat_input': False,\n",
    "    'fc_dropout_ratio': 0.1,\n",
    "    'fc_num_classes': 2,\n",
    "    'fc_align_corners': False,\n",
    "\n",
    "    # training configs\n",
    "    'val_freq': 5,\n",
    "    'print_freq': 100,\n",
    "    'weight_decay': 0,\n",
    "    'manual_seed': 2023,\n",
    "    'num_epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'lr_decay': \"step\",\n",
    "    'gamma': 0.1,\n",
    "    'step': [50],\n",
    "\n",
    "\n",
    "    'name': 'region_gen',\n",
    "    'use_tb_logger': True,\n",
    "    'debug_path': False,\n",
    "    'set_CUDA_VISIBLE_DEVICES': True,\n",
    "    'gpu_ids': [0],\n",
    "\n",
    "    # dataset configs\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    'mask_dir': '/path/to/DFMM-Spotlight/mask',\n",
    "    'train_img_dir': '/path/to/DFMM-Spotlight/train_images',\n",
    "    'test_img_dir': '/path/to/DFMM-Spotlight/test_images',\n",
    "    'train_ann_file': '/path/to/DFMM-Spotlight/mask_ann/train_ann_file.jsonl',\n",
    "    'test_ann_file': '/path/to/DFMM-Spotlight/mask_ann/test_ann_file.jsonl',\n",
    "    'downsample_factor': 2,\n",
    "\n",
    "    # model configs\n",
    "\n",
    "\n",
    "    \"downsample_factor\": 2,\n",
    "\n",
    "    \"model_type\": \"ERLM\",\n",
    "    \"text_embedding_dim\": 512,\n",
    "    \"encoder_in_channels\": 3,\n",
    "    \"fc_in_channels\": 64,\n",
    "    \"fc_in_index\": 4,\n",
    "    \"fc_channels\": 64,\n",
    "    \"fc_num_convs\": 1,\n",
    "    \"fc_concat_input\": False,\n",
    "    \"fc_dropout_ratio\": 0.1,\n",
    "    \"fc_num_classes\": 2,\n",
    "    \"fc_align_corners\": False,\n",
    "\n",
    "    # training configs\n",
    "    \"val_freq\": 5,\n",
    "    \"print_freq\": 100,\n",
    "    \"weight_decay\": 0,\n",
    "    \"manual_seed\": 2023,\n",
    "    \"num_epochs\": 100,\n",
    "    \"lr\": \"!!float 1e-4\", # idk what this means\n",
    "    \"lr_decay\": \"step\",\n",
    "    \"gamma\": 0.1,\n",
    "    \"step\": 50,\n",
    "\n",
    "    #text prompt\n",
    "    \"text_prompt\": \"a blue dress\", # we can change this to any text prompt we want\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline Function\n",
    "This function starts by parsing all the arguments. It takes in the image path,text promt, output path, and model path. It also loads the elrm model using the specifics in the yaml file. We will likely convert the yaml file to a code cell and call it in this function. \n",
    "\n",
    "Then it calls the load_image function, and converts the output to a tensor. \n",
    "\n",
    "It also takes in the text prompt and tokenizes it.\n",
    "\n",
    "The function uses the CLIP model to encode the text, and both the image and text embeddings are passed throught the encoder.\n",
    "The decoder uses `argmax`` to create a segmentation mask and converts the image to grayscale.\n",
    "\n",
    "Then the stable diffusion model is then loaded and transfered to GPU.\n",
    "The model generates an image using the input image, the segmentation mask from the elrm model and the text prompt. The diffusion model is run for 50 steps. ( mores teps = high quality image, slow, fewer steps = faster, lower quality image, more noise)\n",
    "\n",
    "Then the generated image is blended with the input image and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #parse arguments from dictionary\n",
    "    opt = parse(options_dict, is_train=True)\n",
    "    # convert to NoneDict, which returns None for missing keys\n",
    "    opt = dict_to_nonedict(opt) # this is another custom fucntion from the utils module\n",
    "\n",
    "    #load model cnbfiguration\n",
    "    model = create_model(opt) #create model from options (need create_model function from models module))\n",
    "    model.load_network()\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "\n",
    "    #load image\n",
    "    img = load_image(opt['img_path'])\n",
    "    #conver to tensor\n",
    "    img= torch.from_numpy(img).unsqueeze(dim=0).to(model.device)\n",
    "    \n",
    "    #load text inputs with clip to encode text\n",
    "    text_input = torch.cat([clip.tokenize(opt[\"text prompt\"])]).to(model.device)\n",
    "\n",
    "    #pass image and text through encoder\n",
    "    with torch.no_grad():\n",
    "        # text embedding\n",
    "        text_embedding = model.clip.encode_text(text_input)\n",
    "        #encode text\n",
    "        text_encoding = model.encoder(img, text_embedding)\n",
    "        seg_logits = model.decoder(text_encoding)\n",
    "\n",
    "    #argmax to get segmentation map\n",
    "    seg_pred = seg_logits.argmax(dim=1).cpu().numpy()[0]\n",
    "    #convert to grayscale image\n",
    "    seg_img = Image.fromarray(np.uint8(seg_pred * 255))\n",
    "\n",
    "    img = Image.open(opt['img_path']).convert(\"RGB\").resize((256, 512))\n",
    "\n",
    "    # Load pipeline\n",
    "    #load stable diffusion inpaingting model (we can change this if we want)\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        opt['texfit_model_path'], revision=\"fp16\",\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "        #move model to gpu\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    #generate image\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(2023)\n",
    "    images = pipe(\n",
    "        height=512,\n",
    "        width=256,\n",
    "        prompt=[opt['text_prompt']],\n",
    "        image=img,\n",
    "        mask_image=seg_img,\n",
    "        num_inference_steps=50,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "    #blend generated image with original image\n",
    "    final_img = Image.composite(images[0], img, seg_img)\n",
    "    #save image\n",
    "    final_img.save(opt['output_path'])\n",
    "    print('Saved edited result to', opt['output_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call main() directly in the notebook\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
