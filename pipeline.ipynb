{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes the images and XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Created Modules \n",
    "These modules are in the textfit github. In order for this code to fully function, we would need to include the classes and functions in this notebook, or keep them as modules in our repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import create_model\n",
    "#from utils.options import dict_to_nonedict, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images\n",
    "\n",
    "This code loads an image and downsamples it by a factor of 2 (to reduce computational cost WE COULD DO MORE?), resizes it, converts the image into a a numpy array, and returns float32 values in in the numpy array.\n",
    "This prepares the images to be loaded into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image\n",
    "def load_image(image_path):\n",
    "    #set downsample factor\n",
    "    downsample_factor = 2\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        #open image\n",
    "        image = Image.open(f)\n",
    "        width, height = image.size\n",
    "        #donwsample image\n",
    "        width = width // downsample_factor\n",
    "        height = height // downsample_factor\n",
    "        #resize image\n",
    "        image = image.resize(size=(width, height), resample=Image.NEAREST)\n",
    "        #transpose array\n",
    "        image = np.array(image).transpose(2, 0, 1)\n",
    "    #return array\n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline Function\n",
    "This function starts by parsing all the arguments. It takes in the image path,text promt, output path, and model path. It also loads the elrm model using the specifics in the yaml file. We will likely convert the yaml file to a code cell and call it in this function. \n",
    "\n",
    "Then it calls the load_image function, and converts the output to a tensor. \n",
    "\n",
    "It also takes in the text prompt and tokenizes it.\n",
    "\n",
    "The function uses the CLIP model to encode the text, and both the image and text embeddings are passed throught the encoder.\n",
    "The decoder uses `argmax`` to create a segmentation mask and converts the image to grayscale.\n",
    "\n",
    "Then the stable diffusion model is then loaded and transfered to GPU.\n",
    "The model generates an image using the input image, the segmentation mask from the elrm model and the text prompt. The diffusion model is run for 50 steps. ( mores teps = high quality image, slow, fewer steps = faster, lower quality image, more noise)\n",
    "\n",
    "Then the generated image is blended with the input image and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of yaml file converted to dictionary?\n",
    "name: region_gen\n",
    "use_tb_logger: true\n",
    "debug_path: False\n",
    "set_CUDA_VISIBLE_DEVICES: True\n",
    "gpu_ids: [0]\n",
    "\n",
    "# dataset configs\n",
    "batch_size: 8\n",
    "num_workers: 4\n",
    "mask_dir: /path/to/DFMM-Spotlight/mask\n",
    "train_img_dir: /path/to/DFMM-Spotlight/train_images\n",
    "test_img_dir: /path/to/DFMM-Spotlight/test_images\n",
    "train_ann_file: /path/to/DFMM-Spotlight/mask_ann/train_ann_file.jsonl\n",
    "test_ann_file: /path/to/DFMM-Spotlight/mask_ann/test_ann_file.jsonl\n",
    "downsample_factor: 2\n",
    "\n",
    "model_type: ERLM\n",
    "text_embedding_dim: 512\n",
    "encoder_in_channels: 3\n",
    "fc_in_channels: 64\n",
    "fc_in_index: 4\n",
    "fc_channels: 64\n",
    "fc_num_convs: 1\n",
    "fc_concat_input: False\n",
    "fc_dropout_ratio: 0.1\n",
    "fc_num_classes: 2\n",
    "fc_align_corners: False\n",
    "\n",
    "# training configs\n",
    "val_freq: 5\n",
    "print_freq: 100\n",
    "weight_decay: 0\n",
    "manual_seed: 2023\n",
    "num_epochs: 100\n",
    "lr: !!float 1e-4\n",
    "lr_decay: step\n",
    "gamma: 0.1\n",
    "step: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
