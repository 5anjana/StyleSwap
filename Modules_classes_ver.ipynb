{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9c2684",
   "metadata": {},
   "source": [
    "## Models (class version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4a32f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashle\\miniconda3\\envs\\Orientation_env\\Lib\\site-packages\\mmengine\\optim\\optimizer\\zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "# archs\n",
    "# fcn archs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.cnn import ConvModule #, normal_init\n",
    "# from mmseg.ops import resize\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e299316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDecodeHead(nn.Module):\n",
    "    \"\"\"Base class for BaseDecodeHead.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int|Sequence[int]): Input channels.\n",
    "        channels (int): Channels after modules, before conv_seg.\n",
    "        num_classes (int): Number of classes.\n",
    "        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n",
    "        conv_cfg (dict|None): Config of conv layers. Default: None.\n",
    "        norm_cfg (dict|None): Config of norm layers. Default: None.\n",
    "        act_cfg (dict): Config of activation layers.\n",
    "            Default: dict(type='ReLU')\n",
    "        in_index (int|Sequence[int]): Input feature index. Default: -1\n",
    "        input_transform (str|None): Transformation type of input features.\n",
    "            Options: 'resize_concat', 'multiple_select', None.\n",
    "            'resize_concat': Multiple feature maps will be resize to the\n",
    "                same size as first one and than concat together.\n",
    "                Usually used in FCN head of HRNet.\n",
    "            'multiple_select': Multiple feature maps will be bundle into\n",
    "                a list and passed into decode head.\n",
    "            None: Only one select feature map is allowed.\n",
    "            Default: None.\n",
    "        loss_decode (dict): Config of decode loss.\n",
    "            Default: dict(type='CrossEntropyLoss').\n",
    "        ignore_index (int | None): The label index to be ignored. When using\n",
    "            masked BCE loss, ignore_index should be set to None. Default: 255\n",
    "        sampler (dict|None): The config of segmentation map sampler.\n",
    "            Default: None.\n",
    "        align_corners (bool): align_corners argument of F.interpolate.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False):\n",
    "        super(BaseDecodeHead, self).__init__()\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Extra repr.\"\"\"\n",
    "        s = f'input_transform={self.input_transform}, ' \\\n",
    "            f'ignore_index={self.ignore_index}, ' \\\n",
    "            f'align_corners={self.align_corners}'\n",
    "        return s\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        # normal_init(self.conv_seg, mean=0, std=0.01) don't have normal_init library\n",
    "\n",
    "        init.normal_(self.conv_seg.weight, mean=0, std=0.01)\n",
    "        if self.conv_seg.bias is not None:\n",
    "            init.constant_(self.conv_seg.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    input=x,\n",
    "                    size=inputs[0].shape[2:], # resizes to the spatial size of the first feature map\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Placeholder of forward function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cls_seg(self, feat):\n",
    "        \"\"\"Classify each pixel.\"\"\"\n",
    "        if self.dropout is not None:\n",
    "            feat = self.dropout(feat)\n",
    "        output = self.conv_seg(feat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19600c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNHead(BaseDecodeHead):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 **kwargs):\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        super(FCNHead, self).__init__(**kwargs)\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ConvModule(\n",
    "                self.in_channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg))\n",
    "        for i in range(num_convs - 1):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "        if num_convs == 0:\n",
    "            self.convs = nn.Identity()\n",
    "        else:\n",
    "            self.convs = nn.Sequential(*convs)\n",
    "        if self.concat_input:\n",
    "            self.conv_cat = ConvModule(\n",
    "                self.in_channels + self.channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "        output = self.convs(x)\n",
    "        if self.concat_input:\n",
    "            output = self.conv_cat(torch.cat([x, output], dim=1))\n",
    "        output = self.cls_seg(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa10005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadFCNHead(nn.Module):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 num_head=18,\n",
    "                 **kwargs):\n",
    "        super(MultiHeadFCNHead, self).__init__()\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "\n",
    "        conv_seg_head_list = []\n",
    "        for _ in range(self.num_head):\n",
    "            conv_seg_head_list.append(\n",
    "                nn.Conv2d(channels, num_classes, kernel_size=1))\n",
    "\n",
    "        self.conv_seg_head_list = nn.ModuleList(conv_seg_head_list)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs_list = []\n",
    "        conv_cat_list = []\n",
    "\n",
    "        for _ in range(self.num_head):\n",
    "            convs = []\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.in_channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "            for _ in range(num_convs - 1):\n",
    "                convs.append(\n",
    "                    ConvModule(\n",
    "                        self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "            if num_convs == 0:\n",
    "                convs_list.append(nn.Identity())\n",
    "            else:\n",
    "                convs_list.append(nn.Sequential(*convs))\n",
    "            if self.concat_input:\n",
    "                conv_cat_list.append(\n",
    "                    ConvModule(\n",
    "                        self.in_channels + self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "\n",
    "        self.convs_list = nn.ModuleList(convs_list)\n",
    "        self.conv_cat_list = nn.ModuleList(conv_cat_list)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "\n",
    "        output_list = []\n",
    "        for head_idx in range(self.num_head):\n",
    "            output = self.convs_list[head_idx](x)\n",
    "            if self.concat_input:\n",
    "                output = self.conv_cat_list[head_idx](\n",
    "                    torch.cat([x, output], dim=1))\n",
    "            if self.dropout is not None:\n",
    "                output = self.dropout(output)\n",
    "            output = self.conv_seg_head_list[head_idx](output)\n",
    "            output_list.append(output)\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        for conv_seg_head in self.conv_seg_head_list:\n",
    "            # normal_init(conv_seg_head, mean=0, std=0.01) don't have package\n",
    "            init.normal_(conv_seg_head.weight, mean=0, std=0.01)\n",
    "            if conv_seg_head.bias is not None:\n",
    "                init.constant_(conv_seg_head.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    x,\n",
    "                    size=inputs[0].shape[2:],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5944c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet arch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import (ConvModule, build_activation_layer,\n",
    "                      build_norm_layer, build_upsample_layer)\n",
    "# from mmcv.runner import load_checkpoint\n",
    "# from mmcv.utils.parrots_wrapper import _BatchNorm\n",
    "# from mmseg.utils import get_root_logger\n",
    "# UPSAMPLE_LAYERS, constant_init, kaiming_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7203bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConvBlock(nn.Module):\n",
    "    \"\"\"Upsample convolution block in decoder for UNet.\n",
    "\n",
    "    This upsample convolution block consists of one upsample module\n",
    "    followed by one convolution block. The upsample module expands the\n",
    "    high-level low-resolution feature map and the convolution block fuses\n",
    "    the upsampled high-level low-resolution feature map and the low-level\n",
    "    high-resolution feature map from encoder.\n",
    "\n",
    "    Args:\n",
    "        conv_block (nn.Sequential): Sequential of convolutional layers.\n",
    "        in_channels (int): Number of input channels of the high-level\n",
    "        skip_channels (int): Number of input channels of the low-level\n",
    "        high-resolution feature map from encoder.\n",
    "        out_channels (int): Number of output channels.\n",
    "        num_convs (int): Number of convolutional layers in the conv_block.\n",
    "            Default: 2.\n",
    "        stride (int): Stride of convolutional layer in conv_block. Default: 1.\n",
    "        dilation (int): Dilation rate of convolutional layer in conv_block.\n",
    "            Default: 1.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv'). If the size of\n",
    "            high-level feature map is the same as that of skip feature map\n",
    "            (low-level feature map from encoder), it does not need upsample the\n",
    "            high-level feature map and the upsample_cfg is None.\n",
    "        dcn (bool): Use deformable convoluton in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 conv_block,\n",
    "                 in_channels,\n",
    "                 skip_channels,\n",
    "                 out_channels,\n",
    "                 num_convs=2,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "\n",
    "        self.conv_block = conv_block(\n",
    "            in_channels=2 * skip_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_convs=num_convs,\n",
    "            stride=stride,\n",
    "            dilation=dilation,\n",
    "            with_cp=with_cp,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg,\n",
    "            dcn=None,\n",
    "            plugins=None)\n",
    "        if upsample_cfg is not None:\n",
    "            self.upsample = build_upsample_layer(\n",
    "                cfg=upsample_cfg,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=skip_channels,\n",
    "                with_cp=with_cp,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg)\n",
    "        else:\n",
    "            self.upsample = ConvModule(\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg)\n",
    "\n",
    "    def forward(self, skip, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        out = torch.cat([skip, x], dim=1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3028c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block for UNet.\n",
    "\n",
    "    This module consists of several plain convolutional layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        num_convs (int): Number of convolutional layers. Default: 2.\n",
    "        stride (int): Whether use stride convolution to downsample\n",
    "            the input feature map. If stride=2, it only uses stride convolution\n",
    "            in the first convolutional layer to downsample the input feature\n",
    "            map. Options are 1 or 2. Default: 1.\n",
    "        dilation (int): Whether use dilated convolution to expand the\n",
    "            receptive field. Set dilation rate of each convolutional layer and\n",
    "            the dilation rate of the first convolutional layer is always 1.\n",
    "            Default: 1.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        dcn (bool): Use deformable convoluton in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_convs=2,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "        convs = []\n",
    "        for i in range(num_convs):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    in_channels=in_channels if i == 0 else out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride if i == 0 else 1,\n",
    "                    dilation=1 if i == 0 else dilation,\n",
    "                    padding=1 if i == 0 else dilation,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg))\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.convs, x)\n",
    "        else:\n",
    "            out = self.convs(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a794a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvModule(nn.Module):\n",
    "    \"\"\"Deconvolution upsample module in decoder for UNet (2X upsample).\n",
    "\n",
    "    This module uses deconvolution to upsample feature map in the decoder\n",
    "    of UNet.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 4.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 *,\n",
    "                 kernel_size=4,\n",
    "                 scale_factor=2):\n",
    "        super(DeconvModule, self).__init__()\n",
    "\n",
    "        assert (kernel_size - scale_factor >= 0) and\\\n",
    "               (kernel_size - scale_factor) % 2 == 0,\\\n",
    "               f'kernel_size should be greater than or equal to scale_factor '\\\n",
    "               f'and (kernel_size - scale_factor) should be even numbers, '\\\n",
    "               f'while the kernel size is {kernel_size} and scale_factor is '\\\n",
    "               f'{scale_factor}.'\n",
    "\n",
    "        stride = scale_factor\n",
    "        padding = (kernel_size - scale_factor) // 2\n",
    "        self.with_cp = with_cp\n",
    "        deconv = nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding)\n",
    "\n",
    "        norm_name, norm = build_norm_layer(norm_cfg, out_channels)\n",
    "        activate = build_activation_layer(act_cfg)\n",
    "        self.deconv_upsamping = nn.Sequential(deconv, norm, activate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.deconv_upsamping, x)\n",
    "        else:\n",
    "            out = self.deconv_upsamping(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3237be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpConv(nn.Module):\n",
    "    \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "    This module uses interpolation to upsample feature map in the decoder\n",
    "    of UNet. It consists of one interpolation upsample layer and one\n",
    "    convolutional layer. It can be one interpolation upsample layer followed\n",
    "    by one convolutional layer (conv_first=False) or one convolutional layer\n",
    "    followed by one interpolation upsample layer (conv_first=True).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        conv_first (bool): Whether convolutional layer or interpolation\n",
    "            upsample layer first. Default: False. It means interpolation\n",
    "            upsample layer followed by one convolutional layer.\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "        stride (int): Stride of the convolutional layer. Default: 1.\n",
    "        padding (int): Padding of the convolutional layer. Default: 1.\n",
    "        upsampe_cfg (dict): Interpolation config of the upsample layer.\n",
    "            Default: dict(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_layer=nn.BatchNorm2d,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 conv_first=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 upsample_cfg=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        if upsample_cfg is None:\n",
    "            upsample_cfg=dict(scale_factor=2, model='bilinear', align_corners=False)\n",
    "        \n",
    "        layers = []\n",
    "        conv = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if norm_layer is not None:\n",
    "            conv.append(norm_layer(out_channels))\n",
    "        if act_layer is not None:\n",
    "            conv.append(act_layer())\n",
    "        \n",
    "        conv_block = nn.Sequential(*conv)\n",
    "        upsample_layer = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        if conv_first:\n",
    "            layers = [conv_block, upsample_layer]\n",
    "        else:\n",
    "            layers = [upsample_layer, conv_block]\n",
    "        \n",
    "        self.interp_upsample = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.interp_upsample, x)\n",
    "        return self.interp_upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b11082e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet and atrunet\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet backbone.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondence encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondence encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convolution in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(UNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_outs = []\n",
    "\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            # logger = get_root_logger() library issue\n",
    "            # load_checkpoint(self, pretrained, strict=False, logger=logger) library issue\n",
    "\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            self.load_state_dict(state_dict, strcit=False)\n",
    "            print(f\"Loaded pretrained weights from {pretrained}\")\n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    # kaiming_init(m) library issue\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc31a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrUNet(nn.Module):\n",
    "    \"\"\"ShapeUNet backbone with small modifications.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondance encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondance encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convoluton in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 attr_embedding=128,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(AttrUNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels + attr_embedding,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x, attr_embedding):\n",
    "        enc_outs = []\n",
    "        Be, Ce = attr_embedding.size()\n",
    "        for enc in self.encoder:\n",
    "            _, _, H, W = x.size()\n",
    "            x = enc(\n",
    "                torch.cat([\n",
    "                    x,\n",
    "                    attr_embedding.view(Be, Ce, 1, 1).expand((Be, Ce, H, W))\n",
    "                ],\n",
    "                          dim=1))\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"Loaded pretrained wieghts from {pretrained}\")\n",
    "            if missing_keys:\n",
    "                print(f\"Missing keys: {missing_keys}\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "        \n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orientation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
