{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text prompt\n",
    "text_promt = 'pink vertical striped pants'\n",
    "\n",
    "#model paths\n",
    "elrm_model_path = '../region_generation_epoch60.pth' \n",
    "styleswap_model_path = '../fashion-model-finetuned_full'\n",
    "output_path = 'example_output2.png'\n",
    "img_path = '../resized_ash.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target, topk=1, thresh=None):\n",
    "    \"\"\"Calculate accuracy according to the prediction and target.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The model prediction, shape (N, num_class, ...)\n",
    "        target (torch.Tensor): The target of each prediction, shape (N, , ...)\n",
    "        topk (int | tuple[int], optional): If the predictions in ``topk``\n",
    "            matches the target, the predictions will be regarded as\n",
    "            correct ones. Defaults to 1.\n",
    "        thresh (float, optional): If not None, predictions with scores under\n",
    "            this threshold are considered incorrect. Default to None.\n",
    "\n",
    "    Returns:\n",
    "        float | tuple[float]: If the input ``topk`` is a single integer,\n",
    "            the function will return a single float as accuracy. If\n",
    "            ``topk`` is a tuple containing multiple integers, the\n",
    "            function will return a tuple containing accuracies of\n",
    "            each ``topk`` number.\n",
    "    \"\"\"\n",
    "    assert isinstance(topk, (int, tuple)) # topk should be int or tuple \n",
    "    if isinstance(topk, int):\n",
    "        topk = (topk, )\n",
    "        return_single = True\n",
    "    else:\n",
    "        return_single = False # convert topk to tuple if int, track how many values user passed\n",
    "\n",
    "    maxk = max(topk) # max number of top predictions we'll evaluate\n",
    "    if pred.size(0) == 0:\n",
    "        accu = [pred.new_tensor(0.) for i in range(len(topk))]\n",
    "        return accu[0] if return_single else accu # check if pred batch is empty\n",
    "    assert pred.ndim == target.ndim + 1 # checks that pred has one more dimension than target\n",
    "    assert pred.size(0) == target.size(0) # same size\n",
    "    assert maxk <= pred.size(1), \\\n",
    "        f'maxk {maxk} exceeds pred dimension {pred.size(1)}'\n",
    "    pred_value, pred_label = pred.topk(maxk, dim=1) # selects topk predictions and their indices\n",
    "    # transpose to shape (maxk, N, ...)\n",
    "    pred_label = pred_label.transpose(0, 1)\n",
    "    correct = pred_label.eq(target.unsqueeze(0).expand_as(pred_label)) # makes correct a boolean matrix (whether top-k predictions match the target)\n",
    "    if thresh is not None:\n",
    "        # Only prediction values larger than thresh are counted as correct\n",
    "        correct = correct & (pred_value > thresh).t() # masks out prediction below threshold with top-k scores\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / target.numel()))\n",
    "    return res[0] if return_single else res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_loss(loss, reduction):\n",
    "    \"\"\"Reduce loss as specified.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Elementwise loss tensor.\n",
    "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
    "\n",
    "    Return:\n",
    "        Tensor: Reduced loss tensor.\n",
    "    \"\"\"\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch.\n",
    "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        assert weight.dim() == loss.dim()\n",
    "        if weight.dim() > 1:\n",
    "            assert weight.size(1) == 1 or weight.size(1) == loss.size(1)\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != 'none':\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred,\n",
    "                  label,\n",
    "                  weight=None,\n",
    "                  class_weight=None,\n",
    "                  reduction='mean',\n",
    "                  avg_factor=None,\n",
    "                  ignore_index=-100):\n",
    "    \"\"\"The wrapper function for :func:`F.cross_entropy`\"\"\"\n",
    "    # class_weight is a manual rescaling weight given to each class.\n",
    "    # If given, has to be a Tensor of size C element-wise losses\n",
    "    loss = F.cross_entropy(\n",
    "        pred,\n",
    "        label,\n",
    "        weight=class_weight,\n",
    "        reduction='none',\n",
    "        ignore_index=ignore_index)\n",
    "\n",
    "    # apply weights and do the reduction\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "    loss = weight_reduce_loss(\n",
    "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_onehot_labels(labels, label_weights, target_shape, ignore_index):\n",
    "    \"\"\"Expand onehot labels to match the size of prediction.\"\"\"\n",
    "    bin_labels = labels.new_zeros(target_shape)\n",
    "    valid_mask = (labels >= 0) & (labels != ignore_index)\n",
    "    inds = torch.nonzero(valid_mask, as_tuple=True)\n",
    "\n",
    "    if inds[0].numel() > 0:\n",
    "        if labels.dim() == 3:\n",
    "            bin_labels[inds[0], labels[valid_mask], inds[1], inds[2]] = 1\n",
    "        else:\n",
    "            bin_labels[inds[0], labels[valid_mask]] = 1\n",
    "\n",
    "    valid_mask = valid_mask.unsqueeze(1).expand(target_shape).float()\n",
    "    if label_weights is None:\n",
    "        bin_label_weights = valid_mask\n",
    "    else:\n",
    "        bin_label_weights = label_weights.unsqueeze(1).expand(target_shape)\n",
    "        bin_label_weights *= valid_mask\n",
    "\n",
    "    return bin_labels, bin_label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred,\n",
    "                         label,\n",
    "                         weight=None,\n",
    "                         reduction='mean',\n",
    "                         avg_factor=None,\n",
    "                         class_weight=None,\n",
    "                         ignore_index=255):\n",
    "    \"\"\"Calculate the binary CrossEntropy loss.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, 1).\n",
    "        label (torch.Tensor): The learning label of the prediction.\n",
    "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "        reduction (str, optional): The method used to reduce the loss.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (list[float], optional): The weight for each class.\n",
    "        ignore_index (int | None): The label index to be ignored. Default: 255\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    \"\"\"\n",
    "    if pred.dim() != label.dim():\n",
    "        assert (pred.dim() == 2 and label.dim() == 1) or (\n",
    "                pred.dim() == 4 and label.dim() == 3), \\\n",
    "            'Only pred shape [N, C], label shape [N] or pred shape [N, C, ' \\\n",
    "            'H, W], label shape [N, H, W] are supported'\n",
    "        label, weight = _expand_onehot_labels(label, weight, pred.shape,\n",
    "                                              ignore_index)\n",
    "\n",
    "    # weighted element-wise losses\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, label.float(), pos_weight=class_weight, reduction='none')\n",
    "    # do the reduction for the weighted loss\n",
    "    loss = weight_reduce_loss(\n",
    "        loss, weight, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_cross_entropy(pred,\n",
    "                       target,\n",
    "                       label,\n",
    "                       reduction='mean',\n",
    "                       avg_factor=None,\n",
    "                       class_weight=None,\n",
    "                       ignore_index=None):\n",
    "    \"\"\"Calculate the CrossEntropy loss for masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n",
    "            of classes.\n",
    "        target (torch.Tensor): The learning label of the prediction.\n",
    "        label (torch.Tensor): ``label`` indicates the class label of the mask'\n",
    "            corresponding object. This will be used to select the mask in the\n",
    "            of the class which the object belongs to when the mask prediction\n",
    "            if not class-agnostic.\n",
    "        reduction (str, optional): The method used to reduce the loss.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (list[float], optional): The weight for each class.\n",
    "        ignore_index (None): Placeholder, to be consistent with other loss.\n",
    "            Default: None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    \"\"\"\n",
    "    assert ignore_index is None, 'BCE loss does not support ignore_index'\n",
    "    # TODO: handle these two reserved arguments\n",
    "    assert reduction == 'mean' and avg_factor is None\n",
    "    num_rois = pred.size()[0]\n",
    "    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n",
    "    pred_slice = pred[inds, label].squeeze(1)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        pred_slice, target, weight=class_weight, reduction='mean')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    \"\"\"CrossEntropyLoss.\n",
    "\n",
    "    Args:\n",
    "        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n",
    "            of softmax. Defaults to False.\n",
    "        use_mask (bool, optional): Whether to use mask cross entropy loss.\n",
    "            Defaults to False.\n",
    "        reduction (str, optional): . Defaults to 'mean'.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        class_weight (list[float], optional): Weight of each class.\n",
    "            Defaults to None.\n",
    "        loss_weight (float, optional): Weight of the loss. Defaults to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 use_sigmoid=False,\n",
    "                 use_mask=False,\n",
    "                 reduction='mean',\n",
    "                 class_weight=None,\n",
    "                 loss_weight=1.0):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        assert (use_sigmoid is False) or (use_mask is False)\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.use_mask = use_mask\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "        if self.use_sigmoid:\n",
    "            self.cls_criterion = binary_cross_entropy\n",
    "        elif self.use_mask:\n",
    "            self.cls_criterion = mask_cross_entropy\n",
    "        else:\n",
    "            self.cls_criterion = cross_entropy\n",
    "\n",
    "    def forward(self,\n",
    "                cls_score,\n",
    "                label,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        if self.class_weight is not None:\n",
    "            class_weight = cls_score.new_tensor(self.class_weight)\n",
    "        else:\n",
    "            class_weight = None\n",
    "        loss_cls = self.loss_weight * self.cls_criterion(\n",
    "            cls_score,\n",
    "            label,\n",
    "            weight,\n",
    "            class_weight=class_weight,\n",
    "            reduction=reduction,\n",
    "            avg_factor=avg_factor,\n",
    "            **kwargs)\n",
    "        return loss_cls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def compute_cross_entropy_loss(cls_score,\n",
    "                               label,\n",
    "                               *,\n",
    "                               use_sigmoid=False,\n",
    "                               use_mask=False,\n",
    "                               class_weight=None,\n",
    "                               loss_weight=1.0,\n",
    "                               weight=None,\n",
    "                               avg_factor=None,\n",
    "                               reduction='mean',\n",
    "                               reduction_override=None,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs):\n",
    "    \"\"\"\n",
    "    Functional equivalent of CrossEntropyLoss class forward method.\n",
    "\n",
    "    Args:\n",
    "        cls_score (Tensor): Prediction logits.\n",
    "        label (Tensor): Ground-truth labels.\n",
    "        use_sigmoid (bool): Use sigmoid + BCE.\n",
    "        use_mask (bool): Use mask cross-entropy.\n",
    "        class_weight (list[float] | Tensor | None): Per-class weight.\n",
    "        loss_weight (float): Scalar multiplier on the loss.\n",
    "        weight (Tensor | None): Sample-wise weighting.\n",
    "        avg_factor (float | None): Averaging factor for mean reduction.\n",
    "        reduction (str): 'none' | 'mean' | 'sum'.\n",
    "        reduction_override (str | None): Overrides the reduction method.\n",
    "        ignore_index (int): Label index to ignore.\n",
    "        kwargs: Passed to the specific loss function.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "    \"\"\"\n",
    "    assert not (use_sigmoid and use_mask), \\\n",
    "        \"Cannot use both sigmoid and mask mode.\"\n",
    "\n",
    "    reduction = reduction_override if reduction_override else reduction\n",
    "\n",
    "    # Select the appropriate core loss function\n",
    "    if use_sigmoid:\n",
    "        loss_fn = binary_cross_entropy\n",
    "    elif use_mask:\n",
    "        loss_fn = mask_cross_entropy\n",
    "    else:\n",
    "        loss_fn = cross_entropy\n",
    "\n",
    "    # Convert class weights to tensor if needed\n",
    "    if class_weight is not None and not torch.is_tensor(class_weight):\n",
    "        class_weight = cls_score.new_tensor(class_weight)\n",
    "\n",
    "    return loss_weight * loss_fn(\n",
    "        cls_score,\n",
    "        label,\n",
    "        weight=weight,\n",
    "        class_weight=class_weight,\n",
    "        reduction=reduction,\n",
    "        avg_factor=avg_factor,\n",
    "        ignore_index=ignore_index,\n",
    "        **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet arch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import (ConvModule, build_activation_layer,\n",
    "                      build_norm_layer, build_upsample_layer)\n",
    "# from mmcv.runner import load_checkpoint\n",
    "# from mmcv.utils.parrots_wrapper import _BatchNorm\n",
    "# from mmseg.utils import get_root_logger\n",
    "# UPSAMPLE_LAYERS, constant_init, kaiming_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_class\n",
    "import torch\n",
    "from mmcv.cnn import ConvModule, build_upsample_layer\n",
    "\n",
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        conv_block,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        num_convs=2,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        with_cp=False,\n",
    "        conv_cfg=None,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        upsample_cfg=dict(type='InterpConv'),\n",
    "        dcn=None,\n",
    "        plugins=None\n",
    "        ):\n",
    "        \"\"\"Builds the upsample and conv blocks used in UNet decoder.\"\"\"\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "\n",
    "        self.conv_block = conv_block(\n",
    "            in_channels=2 * skip_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_convs=num_convs,\n",
    "            stride=stride,\n",
    "            dilation=dilation,\n",
    "            with_cp=with_cp,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg,\n",
    "            dcn=None,\n",
    "            plugins=None\n",
    "        )\n",
    "\n",
    "        if upsample_cfg is not None:\n",
    "            print(f\"Building UpConvBlock with upsample_cfg: {upsample_cfg}\")\n",
    "\n",
    "            self.upsample = build_upsample_layer(\n",
    "                cfg=upsample_cfg,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=skip_channels,\n",
    "                with_cp=with_cp,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "            print(f\"Upsample layer built: {self.upsample}\")\n",
    "        else:\n",
    "            self.upsample = ConvModule(\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, skip, x):\n",
    "        \"\"\"Forward function for upsample + conv block.\"\"\"\n",
    "        x = self.upsample(x)\n",
    "        out = torch.cat([skip, x], dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import ConvModule\n",
    "\n",
    "\n",
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_convs=2,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        with_cp=False,\n",
    "        conv_cfg=None,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        dcn=None,\n",
    "        plugins=None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Builds a basic convolutional block for UNet.\n",
    "\n",
    "        This block consists of several plain convolutional layers (Conv + Norm + Activation).\n",
    "\n",
    "        Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        num_convs (int): Number of convolutional layers. Default: 2.\n",
    "        stride (int): If stride=2, applies stride convolution in the first layer. Default: 1.\n",
    "        dilation (int): Dilation rate for all conv layers except the first. Default: 1.\n",
    "        with_cp (bool): If True, enables checkpointing for memory savings. Default: False.\n",
    "        conv_cfg (dict | None): Configuration for convolution layer. Default: None.\n",
    "        norm_cfg (dict | None): Configuration for normalization layer. Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Configuration for activation function. Default: dict(type='ReLU').\n",
    "        dcn (bool): Deformable convolution support. Not implemented. Default: None.\n",
    "        plugins (dict): Plugins for conv layers. Not implemented. Default: None.\n",
    "\n",
    "        Returns:\n",
    "        nn.Sequential: A sequential module containing the convolutional layers.\n",
    "        bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        \n",
    "        self.with_cp = with_cp\n",
    "        convs = []\n",
    "        for i in range(num_convs):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    in_channels=in_channels if i == 0 else out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride if i == 0 else 1,\n",
    "                    dilation=1 if i == 0 else dilation,\n",
    "                    padding=1 if i == 0 else dilation,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for basic convolutional block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            convs (nn.Sequential): Convolutional layers.\n",
    "            with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying convs.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.convs, x)\n",
    "        else:\n",
    "            out = self.convs(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import ConvModule\n",
    "from mmcv.cnn import UPSAMPLE_LAYERS\n",
    "\n",
    "@UPSAMPLE_LAYERS.register_module()\n",
    "class InterpConv(nn.Module):\n",
    "    \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "    This module uses interpolation to upsample feature map in the decoder\n",
    "    of UNet. It consists of one interpolation upsample layer and one\n",
    "    convolutional layer. It can be one interpolation upsample layer followed\n",
    "    by one convolutional layer (conv_first=False) or one convolutional layer\n",
    "    followed by one interpolation upsample layer (conv_first=True).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        conv_first (bool): Whether convolutional layer or interpolation\n",
    "            upsample layer first. Default: False. It means interpolation\n",
    "            upsample layer followed by one convolutional layer.\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "        stride (int): Stride of the convolutional layer. Default: 1.\n",
    "        padding (int): Padding of the convolutional layer. Default: 1.\n",
    "        upsampe_cfg (dict): Interpolation config of the upsample layer.\n",
    "            Default: dict(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 *,\n",
    "                 conv_cfg=None,\n",
    "                 conv_first=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 upsampe_cfg=dict(\n",
    "                     scale_factor=2, mode='bilinear', align_corners=False)):\n",
    "        super(InterpConv, self).__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "        conv = ConvModule(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg)\n",
    "        upsample = nn.Upsample(**upsampe_cfg)\n",
    "        if conv_first:\n",
    "            self.interp_upsample = nn.Sequential(conv, upsample)\n",
    "        else:\n",
    "            self.interp_upsample = nn.Sequential(upsample, conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.interp_upsample, x)\n",
    "        else:\n",
    "            out = self.interp_upsample(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.cnn import UPSAMPLE_LAYERS, ConvModule\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "\n",
    "if 'InterpConv' not in UPSAMPLE_LAYERS:\n",
    "    @UPSAMPLE_LAYERS.register_module()\n",
    "    class InterpConv(nn.Module):\n",
    "        \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "        This module uses interpolation to upsample feature maps in the decoder\n",
    "        of UNet. It consists of one interpolation upsample layer and one\n",
    "        convolutional layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            with_cp (bool): Use checkpoint or not. Default: False.\n",
    "            norm_cfg (dict): Config dict for normalization layer. Default: dict(type='BN').\n",
    "            act_cfg (dict): Config dict for activation layer. Default: dict(type='ReLU').\n",
    "            conv_cfg (dict | None): Config dict for convolution layer. Default: None.\n",
    "            conv_first (bool): Whether convolutional layer or interpolation\n",
    "                upsample layer comes first. Default: False.\n",
    "            kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "            stride (int): Stride of the convolutional layer. Default: 1.\n",
    "            padding (int): Padding of the convolutional layer. Default: 0.\n",
    "            upsample_cfg (dict): Interpolation config of the upsample layer.\n",
    "                Default: dict(scale_factor=2, mode='bilinear', align_corners=False).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     in_channels,\n",
    "                     out_channels,\n",
    "                     with_cp=False,\n",
    "                     norm_cfg=dict(type='BN'),\n",
    "                     act_cfg=dict(type='ReLU'),\n",
    "                     *,\n",
    "                     conv_cfg=None,\n",
    "                     conv_first=False,\n",
    "                     kernel_size=1,\n",
    "                     stride=1,\n",
    "                     padding=0,\n",
    "                     upsample_cfg=dict(scale_factor=2, mode='bilinear', align_corners=False)):\n",
    "            super().__init__()  # Correctly initialize the parent class\n",
    "\n",
    "            self.with_cp = with_cp\n",
    "            print(f\"Initializing InterpConv with in_channels={in_channels}, out_channels={out_channels}\")\n",
    "\n",
    "            # Convolutional layer\n",
    "            conv = ConvModule(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "\n",
    "            # Upsample layer\n",
    "            upsample = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "            # Sequential order\n",
    "            if conv_first:\n",
    "                self.interp_upsample = nn.Sequential(conv, upsample)\n",
    "            else:\n",
    "                self.interp_upsample = nn.Sequential(upsample, conv)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            print(f\"InterpConv forward called with input shape: {x.shape}\")\n",
    "            if self.with_cp and x.requires_grad:\n",
    "                return cp.checkpoint(self.interp_upsample, x)\n",
    "            return self.interp_upsample(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#Other Interpoconv class\n",
    "@UPSAMPLE_LAYERS.register_module()\n",
    "class InterpConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        with_cp=False,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        *,\n",
    "        conv_cfg=None,\n",
    "        conv_first=False,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        upsample_cfg=dict(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "    ):\n",
    "        super(InterpConv, self).__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "        \"\"\"\n",
    "        Builds an interpolation-based upsample module for the UNet decoder.\n",
    "\n",
    "        This module performs interpolation upsampling followed by a convolutional\n",
    "        block, or vice versa depending on `conv_first`.\n",
    "\n",
    "        Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpointing to reduce memory. Default: False.\n",
    "        norm_cfg (dict): Normalization layer config. Default: dict(type='BN').\n",
    "        act_cfg (dict): Activation layer config. Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Convolution config. Default: None.\n",
    "        conv_first (bool): Whether to apply convolution before upsampling. Default: False.\n",
    "        kernel_size (int): Kernel size of the convolution. Default: 1.\n",
    "        stride (int): Stride of the convolution. Default: 1.\n",
    "        padding (int): Padding for the convolution. Default: 0.\n",
    "        upsample_cfg (dict): Config for `nn.Upsample`. Default: bilinear 2x.\n",
    "\n",
    "        Returns:\n",
    "        nn.Sequential: A sequential module (Upsample + Conv or Conv + Upsample).\n",
    "        bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "        conv = ConvModule(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg\n",
    "        )\n",
    "\n",
    "        upsample = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        if conv_first:\n",
    "            self.module = nn.Sequential(conv, upsample)\n",
    "        else:\n",
    "            self.module = nn.Sequential(upsample, conv)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def interp_conv_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for the interpolation-based upsampling module.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        module (nn.Sequential): Upsample + Conv or Conv + Upsample.\n",
    "        with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor after upsampling and convolution.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.module, x)\n",
    "        else:\n",
    "            return self.module(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import build_norm_layer, build_activation_layer\n",
    "\n",
    "\n",
    "class DeconvModule(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        with_cp=False,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        *,\n",
    "        kernel_size=4,\n",
    "        scale_factor=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds a deconvolution upsample module for UNet decoder (2x upsample).\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            with_cp (bool): Whether to use checkpointing. Default: False.\n",
    "            norm_cfg (dict | None): Config dict for normalization layer. Default: dict(type='BN').\n",
    "            act_cfg (dict | None): Config dict for activation function. Default: dict(type='ReLU').\n",
    "            kernel_size (int): Kernel size of the transposed convolution. Default: 4.\n",
    "            scale_factor (int): Upsampling factor (stride). Default: 2.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential module for deconv -> norm -> activation.\n",
    "            bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "        assert (kernel_size - scale_factor >= 0) and \\\n",
    "            (kernel_size - scale_factor) % 2 == 0, (\n",
    "            f'Invalid kernel/scale config: kernel_size={kernel_size}, scale_factor={scale_factor}. '\n",
    "            'kernel_size must be >= scale_factor and their difference must be even.')\n",
    "\n",
    "        stride = scale_factor\n",
    "        padding = (kernel_size - scale_factor) // 2\n",
    "\n",
    "        deconv = nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "        _, norm = build_norm_layer(norm_cfg, out_channels)\n",
    "        activate = build_activation_layer(act_cfg)\n",
    "        \n",
    "\n",
    "        self.module = nn.Sequential(deconv, norm, activate)\n",
    "   \n",
    "\n",
    "\n",
    "    def deconv_module_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for the deconvolution upsampling module.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        module (nn.Sequential): Deconv -> Norm -> Activation module.\n",
    "        with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.module, x)\n",
    "        else:\n",
    "            return self.module(x)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "class InterpConv(nn.Module):\n",
    "    \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "    This module uses interpolation to upsample feature map in the decoder\n",
    "    of UNet. It consists of one interpolation upsample layer and one\n",
    "    convolutional layer. It can be one interpolation upsample layer followed\n",
    "    by one convolutional layer (conv_first=False) or one convolutional layer\n",
    "    followed by one interpolation upsample layer (conv_first=True).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        conv_first (bool): Whether convolutional layer or interpolation\n",
    "            upsample layer first. Default: False. It means interpolation\n",
    "            upsample layer followed by one convolutional layer.\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "        stride (int): Stride of the convolutional layer. Default: 1.\n",
    "        padding (int): Padding of the convolutional layer. Default: 1.\n",
    "        upsampe_cfg (dict): Interpolation config of the upsample layer.\n",
    "            Default: dict(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_layer=nn.BatchNorm2d,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 conv_first=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 upsample_cfg=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        if upsample_cfg is None:\n",
    "            upsample_cfg=dict(scale_factor=2, model='bilinear', align_corners=False)\n",
    "        \n",
    "        layers = []\n",
    "        conv = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if norm_layer is not None:\n",
    "            conv.append(norm_layer(out_channels))\n",
    "        if act_layer is not None:\n",
    "            conv.append(act_layer())\n",
    "        \n",
    "        conv_block = nn.Sequential(*conv)\n",
    "        upsample_layer = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        if conv_first:\n",
    "            layers = [conv_block, upsample_layer]\n",
    "        else:\n",
    "            layers = [upsample_layer, conv_block]\n",
    "        \n",
    "        self.interp_upsample = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.interp_upsample, x)\n",
    "        return self.interp_upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet and atrunet\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet backbone.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondence encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondence encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convolution in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(UNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_outs = []\n",
    "\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            # logger = get_root_logger() library issue\n",
    "            # load_checkpoint(self, pretrained, strict=False, logger=logger) library issue\n",
    "\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            self.load_state_dict(state_dict, strcit=False)\n",
    "            print(f\"Loaded pretrained weights from {pretrained}\")\n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    # kaiming_init(m) library issue\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrUNet(nn.Module):\n",
    "    \"\"\"ShapeUNet backbone with small modifications.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondance encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondance encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convoluton in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 attr_embedding=128,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(AttrUNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels + attr_embedding,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x, attr_embedding):\n",
    "        enc_outs = []\n",
    "        Be, Ce = attr_embedding.size()\n",
    "        for enc in self.encoder:\n",
    "            _, _, H, W = x.size()\n",
    "            x = enc(\n",
    "                torch.cat([\n",
    "                    x,\n",
    "                    attr_embedding.view(Be, Ce, 1, 1).expand((Be, Ce, H, W))\n",
    "                ],\n",
    "                          dim=1))\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"Loaded pretrained wieghts from {pretrained}\")\n",
    "            if missing_keys:\n",
    "                print(f\"Missing keys: {missing_keys}\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "        \n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.cnn import ConvModule #, normal_init\n",
    "# from mmseg.ops import resize\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDecodeHead(nn.Module):\n",
    "    \"\"\"Base class for BaseDecodeHead.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int|Sequence[int]): Input channels.\n",
    "        channels (int): Channels after modules, before conv_seg.\n",
    "        num_classes (int): Number of classes.\n",
    "        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n",
    "        conv_cfg (dict|None): Config of conv layers. Default: None.\n",
    "        norm_cfg (dict|None): Config of norm layers. Default: None.\n",
    "        act_cfg (dict): Config of activation layers.\n",
    "            Default: dict(type='ReLU')\n",
    "        in_index (int|Sequence[int]): Input feature index. Default: -1\n",
    "        input_transform (str|None): Transformation type of input features.\n",
    "            Options: 'resize_concat', 'multiple_select', None.\n",
    "            'resize_concat': Multiple feature maps will be resize to the\n",
    "                same size as first one and than concat together.\n",
    "                Usually used in FCN head of HRNet.\n",
    "            'multiple_select': Multiple feature maps will be bundle into\n",
    "                a list and passed into decode head.\n",
    "            None: Only one select feature map is allowed.\n",
    "            Default: None.\n",
    "        loss_decode (dict): Config of decode loss.\n",
    "            Default: dict(type='CrossEntropyLoss').\n",
    "        ignore_index (int | None): The label index to be ignored. When using\n",
    "            masked BCE loss, ignore_index should be set to None. Default: 255\n",
    "        sampler (dict|None): The config of segmentation map sampler.\n",
    "            Default: None.\n",
    "        align_corners (bool): align_corners argument of F.interpolate.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False):\n",
    "        super(BaseDecodeHead, self).__init__()\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Extra repr.\"\"\"\n",
    "        s = f'input_transform={self.input_transform}, ' \\\n",
    "            f'ignore_index={self.ignore_index}, ' \\\n",
    "            f'align_corners={self.align_corners}'\n",
    "        return s\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        # normal_init(self.conv_seg, mean=0, std=0.01) don't have normal_init library\n",
    "\n",
    "        init.normal_(self.conv_seg.weight, mean=0, std=0.01)\n",
    "        if self.conv_seg.bias is not None:\n",
    "            init.constant_(self.conv_seg.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    input=x,\n",
    "                    size=inputs[0].shape[2:], # resizes to the spatial size of the first feature map\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Placeholder of forward function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cls_seg(self, feat):\n",
    "        \"\"\"Classify each pixel.\"\"\"\n",
    "        if self.dropout is not None:\n",
    "            feat = self.dropout(feat)\n",
    "        output = self.conv_seg(feat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNHead(BaseDecodeHead):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 **kwargs):\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        super(FCNHead, self).__init__(**kwargs)\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ConvModule(\n",
    "                self.in_channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg))\n",
    "        for i in range(num_convs - 1):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "        if num_convs == 0:\n",
    "            self.convs = nn.Identity()\n",
    "        else:\n",
    "            self.convs = nn.Sequential(*convs)\n",
    "        if self.concat_input:\n",
    "            self.conv_cat = ConvModule(\n",
    "                self.in_channels + self.channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "        output = self.convs(x)\n",
    "        if self.concat_input:\n",
    "            output = self.conv_cat(torch.cat([x, output], dim=1))\n",
    "        output = self.cls_seg(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadFCNHead(nn.Module):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 num_head=18,\n",
    "                 **kwargs):\n",
    "        super(MultiHeadFCNHead, self).__init__()\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "\n",
    "        conv_seg_head_list = []\n",
    "        for _ in range(self.num_head):\n",
    "            conv_seg_head_list.append(\n",
    "                nn.Conv2d(channels, num_classes, kernel_size=1))\n",
    "\n",
    "        self.conv_seg_head_list = nn.ModuleList(conv_seg_head_list)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs_list = []\n",
    "        conv_cat_list = []\n",
    "\n",
    "        for _ in range(self.num_head):\n",
    "            convs = []\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.in_channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "            for _ in range(num_convs - 1):\n",
    "                convs.append(\n",
    "                    ConvModule(\n",
    "                        self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "            if num_convs == 0:\n",
    "                convs_list.append(nn.Identity())\n",
    "            else:\n",
    "                convs_list.append(nn.Sequential(*convs))\n",
    "            if self.concat_input:\n",
    "                conv_cat_list.append(\n",
    "                    ConvModule(\n",
    "                        self.in_channels + self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "\n",
    "        self.convs_list = nn.ModuleList(convs_list)\n",
    "        self.conv_cat_list = nn.ModuleList(conv_cat_list)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "\n",
    "        output_list = []\n",
    "        for head_idx in range(self.num_head):\n",
    "            output = self.convs_list[head_idx](x)\n",
    "            if self.concat_input:\n",
    "                output = self.conv_cat_list[head_idx](\n",
    "                    torch.cat([x, output], dim=1))\n",
    "            if self.dropout is not None:\n",
    "                output = self.dropout(output)\n",
    "            output = self.conv_seg_head_list[head_idx](output)\n",
    "            output_list.append(output)\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        for conv_seg_head in self.conv_seg_head_list:\n",
    "            # normal_init(conv_seg_head, mean=0, std=0.01) don't have package\n",
    "            init.normal_(conv_seg_head.weight, mean=0, std=0.01)\n",
    "            if conv_seg_head.bias is not None:\n",
    "                init.constant_(conv_seg_head.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    x,\n",
    "                    size=inputs[0].shape[2:],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELRM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('base')\n",
    "\n",
    "\n",
    "class ERLM():\n",
    "    \"\"\"Editing Region Generation model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.device = torch.device('cpu') #cuda\n",
    "        self.is_train = opt['is_train']\n",
    "\n",
    "        clip_model, _ = clip.load('ViT-B/32', device=torch.device(\"cpu\"))\n",
    "        self.clip = clip_model.to(self.device)\n",
    "        self.encoder = AttrUNet(\n",
    "            in_channels=opt['encoder_in_channels'], attr_embedding=opt['text_embedding_dim']).to(self.device)\n",
    "        self.decoder = FCNHead(\n",
    "            in_channels=opt['fc_in_channels'],\n",
    "            in_index=opt['fc_in_index'],\n",
    "            channels=opt['fc_channels'],\n",
    "            num_convs=opt['fc_num_convs'],\n",
    "            concat_input=opt['fc_concat_input'],\n",
    "            dropout_ratio=opt['fc_dropout_ratio'],\n",
    "            num_classes=opt['fc_num_classes'],\n",
    "            align_corners=opt['fc_align_corners'],\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.init_training_settings()\n",
    "        self.palette = [[0, 0, 0], [255, 255, 255]]\n",
    "\n",
    "    def init_training_settings(self):\n",
    "        optim_params = []\n",
    "\n",
    "        for v in self.encoder.parameters():\n",
    "            if v.requires_grad:\n",
    "                optim_params.append(v)\n",
    "        for v in self.decoder.parameters():\n",
    "            if v.requires_grad:\n",
    "                optim_params.append(v)\n",
    "        # set up optimizers\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            optim_params,\n",
    "            self.opt['lr'],\n",
    "            weight_decay=self.opt['weight_decay'])\n",
    "        self.log_dict = OrderedDict()\n",
    "        self.entropy_loss = CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def feed_data(self, data):\n",
    "        self.image = data['image'].to(self.device)\n",
    "        self.mask = data['mask'].to(self.device)\n",
    "        text = data['text']\n",
    "        text_inputs = torch.cat([clip.tokenize(text)]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.text = self.clip.encode_text(text_inputs)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        self.text_enc = self.encoder(self.image, self.text)\n",
    "        self.seg_logits = self.decoder(self.text_enc)\n",
    "\n",
    "        loss = self.entropy_loss(self.seg_logits, self.mask)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_dict['loss_total'] = loss\n",
    "\n",
    "    def inference(self, data_loader, save_dir):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        acc = 0\n",
    "        num = 0\n",
    "\n",
    "        for _, data in enumerate(data_loader):\n",
    "            image = data['image'].to(self.device)\n",
    "            text = data['text']\n",
    "            text_inputs = torch.cat([clip.tokenize(text)]).to(self.device)\n",
    "            mask = data['mask'].to(self.device)\n",
    "            img_name = data['img_name']\n",
    "\n",
    "            num += image.size(0)\n",
    "            with torch.no_grad():\n",
    "                text_embedding = self.clip.encode_text(text_inputs)\n",
    "                text_enc = self.encoder(image, text_embedding)\n",
    "                seg_logits = self.decoder(text_enc)\n",
    "            seg_pred = seg_logits.argmax(dim=1)\n",
    "            acc += accuracy(seg_logits, mask)\n",
    "            palette_label = self.palette_result(mask.cpu().numpy())\n",
    "            palette_pred = self.palette_result(seg_pred.cpu().numpy())\n",
    "            image_numpy = image[0].cpu().numpy().astype(np.uint8).transpose(1, 2, 0)\n",
    "            image_numpy = image_numpy[..., ::-1]\n",
    "            concat_result = np.concatenate(\n",
    "                (image_numpy, palette_pred, palette_label), axis=1)\n",
    "            img_name_base, img_name_ext = os.path.splitext(img_name[0])\n",
    "            mmcv.imwrite(concat_result, f'{save_dir}/{img_name_base}_{text[0]}{img_name_ext}')\n",
    "\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        return (acc / num).item()\n",
    "\n",
    "    def get_current_log(self):\n",
    "        return self.log_dict\n",
    "\n",
    "    def update_learning_rate(self, epoch):\n",
    "        \"\"\"Update learning rate.\n",
    "\n",
    "        Args:\n",
    "            current_iter (int): Current iteration.\n",
    "            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n",
    "                Default: -1.\n",
    "        \"\"\"\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if self.opt['lr_decay'] == 'step':\n",
    "            lr = self.opt['lr'] * (\n",
    "                self.opt['gamma']**(epoch // self.opt['step']))\n",
    "        elif self.opt['lr_decay'] == 'cos':\n",
    "            lr = self.opt['lr'] * (\n",
    "                1 + math.cos(math.pi * epoch / self.opt['num_epochs'])) / 2\n",
    "        elif self.opt['lr_decay'] == 'linear':\n",
    "            lr = self.opt['lr'] * (1 - epoch / self.opt['num_epochs'])\n",
    "        elif self.opt['lr_decay'] == 'linear2exp':\n",
    "            if epoch < self.opt['turning_point'] + 1:\n",
    "                # learning rate decay as 95%\n",
    "                # at the turning point (1 / 95% = 1.0526)\n",
    "                lr = self.opt['lr'] * (\n",
    "                    1 - epoch / int(self.opt['turning_point'] * 1.0526))\n",
    "            else:\n",
    "                lr *= self.opt['gamma']\n",
    "        elif self.opt['lr_decay'] == 'schedule':\n",
    "            if epoch in self.opt['schedule']:\n",
    "                lr *= self.opt['gamma']\n",
    "        else:\n",
    "            raise ValueError('Unknown lr mode {}'.format(self.opt['lr_decay']))\n",
    "        # set learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    def save_network(self, save_path):\n",
    "        \"\"\"Save networks.\n",
    "        \"\"\"\n",
    "\n",
    "        save_dict = {}\n",
    "        save_dict['encoder'] = self.encoder.state_dict()\n",
    "        save_dict['decoder'] = self.decoder.state_dict()\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "\n",
    "    def load_network(self):\n",
    "        #checkpoint = torch.load(self.opt['elrm_model_path'])  # get correct model path!!! pretrained_model_path\n",
    "\n",
    "        #self.encoder.load_state_dict(\n",
    "         #   checkpoint['encoder'], strict=True)\n",
    "        #self.encoder.eval()\n",
    "\n",
    "        #self.decoder.load_state_dict(\n",
    "         #   checkpoint['decoder'], strict=True)\n",
    "        #self.decoder.eval()\n",
    "        \"\"\"Load the network weights.\"\"\"\n",
    "        # Add map_location to ensure the checkpoint is loaded on the CPU\n",
    "        checkpoint = torch.load(self.opt['elrm_model_path'], map_location=torch.device('cpu'))\n",
    "\n",
    "        self.encoder.load_state_dict(\n",
    "            checkpoint['encoder'], strict=True)\n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.decoder.load_state_dict(\n",
    "            checkpoint['decoder'], strict=True)\n",
    "        self.decoder.eval()\n",
    "\n",
    "    def palette_result(self, result):\n",
    "        seg = result[0]\n",
    "        palette = np.array(self.palette)\n",
    "        assert palette.shape[1] == 3\n",
    "        assert len(palette.shape) == 2\n",
    "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "        for label, color in enumerate(palette):\n",
    "            color_seg[seg == label, :] = color\n",
    "        # convert to BGR\n",
    "        color_seg = color_seg[..., ::-1]\n",
    "        return color_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of yaml file converted to dictionary\n",
    "options_dict = {\n",
    "    'name': 'region_gen',\n",
    "    'use_tb_logger': True,\n",
    "    'debug_path': False,\n",
    "    'set_CUDA_VISIBLE_DEVICES': True,\n",
    "    'gpu_ids': [0],\n",
    "    'is_train' : False,\n",
    "\n",
    "    # dataset configs\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    #change depending on where you have files stored\n",
    "    'mask_dir': '../DFMM-Spotlight/mask',\n",
    "    'train_img_dir': '../DFMM-Spotlight/train_images',\n",
    "    'test_img_dir': '../DFMM-Spotlight/test_images',\n",
    "    'train_ann_file': '../DFMM-Spotlight/mask_ann/train_ann_file.jsonl',\n",
    "    'test_ann_file': '../DFMM-Spotlight/mask_ann/test_ann_file.jsonl',\n",
    "    'downsample_factor': 2,\n",
    "\n",
    "    # model configs\n",
    "    'model_type': 'ERLM',\n",
    "    'text_embedding_dim': 512,\n",
    "    'encoder_in_channels': 3,\n",
    "    'fc_in_channels': 64,\n",
    "    'fc_in_index': 4,\n",
    "    'fc_channels': 64,\n",
    "    'fc_num_convs': 1,\n",
    "    'fc_concat_input': False,\n",
    "    'fc_dropout_ratio': 0.1,\n",
    "    'fc_num_classes': 2,\n",
    "    'fc_align_corners': False,\n",
    "\n",
    "    # training configs\n",
    "    'val_freq': 5,\n",
    "    'print_freq': 100,\n",
    "    'weight_decay': 0,\n",
    "    'manual_seed': 2023,\n",
    "    'num_epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'lr_decay': \"step\",\n",
    "    'gamma': 0.1,\n",
    "    'step': [50],\n",
    "\n",
    "    #text prompt\n",
    "    \"text_prompt\": text_promt,\n",
    "    \n",
    "    #paths (change this to our model paths)\n",
    "    'elrm_model_path' : elrm_model_path, \n",
    "    'styleswap_model_path' : styleswap_model_path,\n",
    "    'output_path' : output_path,\n",
    "    'img_path': img_path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Register the models in a simulated module list\n",
    "# This mimics scanning a folder for *_model.py files\n",
    "model_registry = {\n",
    "    'ERLM': ERLM\n",
    "}\n",
    "\n",
    "# Step 4: Define the dynamic model creation function\n",
    "def create_model(opt):\n",
    "    \"\"\"\n",
    "    Dynamically create a model based on the configuration dictionary.\n",
    "    \n",
    "    Args:\n",
    "        opt (dict): A dictionary containing model configuration. \n",
    "                    Must include 'model_type' key.\n",
    "    \n",
    "    Returns:\n",
    "        model (object): An instance of the specified model class.\n",
    "    \"\"\"\n",
    "    model_type = opt['model_type']\n",
    "    \n",
    "    # Find the model class from the registry\n",
    "    model_cls = model_registry.get(model_type)\n",
    "    \n",
    "    if model_cls is None:\n",
    "        raise ValueError(f\"Model '{model_type}' not found in registry.\")\n",
    "    \n",
    "    # Instantiate the model with options\n",
    "    model = model_cls(opt)\n",
    "    \n",
    "    # Optional logging — for demonstration\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(f\"Model [{model.__class__.__name__}] is created.\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image\n",
    "def load_image(image_path):\n",
    "    #set downsample factor\n",
    "    downsample_factor = 2\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        #open image\n",
    "        image = Image.open(f)\n",
    "        width, height = image.size\n",
    "        #donwsample image\n",
    "        width = width // downsample_factor\n",
    "        height = height // downsample_factor\n",
    "        #resize image\n",
    "        image = image.resize(size=(width, height), resample=Image.NEAREST)\n",
    "        #transpose array\n",
    "        image = np.array(image).transpose(2, 0, 1)\n",
    "    #return array\n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse from utils.options, needed to read the options\n",
    "\n",
    "def parse(options_dict, is_train=True):\n",
    "    \"\"\"Parse options from a dictionary instead of a YAML file.\n",
    "\n",
    "    Args:\n",
    "        options_dict (dict): Dictionary containing model options.\n",
    "        is_train (bool): Indicates whether in training mode. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed options.\n",
    "    \"\"\"\n",
    "    # create copy of input dictionary to avoid modifying the original\n",
    "    opt = options_dict.copy()\n",
    "\n",
    "    #get gpu list\n",
    "    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n",
    "    if opt.get('set_CUDA_VISIBLE_DEVICES', None):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n",
    "        print('export CUDA_VISIBLE_DEVICES=' + gpu_list, flush=True)\n",
    "    else:\n",
    "        print('gpu_list: ', gpu_list, flush=True)\n",
    "\n",
    "    opt['is_train'] = is_train\n",
    "\n",
    "    #save optional paths (only if user wants to save logs & models)\n",
    "    opt['path'] = {}\n",
    "    opt['path']['root'] = os.getcwd() #set root as current working directory\n",
    "\n",
    "    if is_train:\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['models'] = opt.get('models_path', os.path.join(opt['path']['root'], 'models'))\n",
    "        opt['path']['logs'] = opt.get('logs_path', os.path.join(opt['path']['root'], 'logs'))\n",
    "        opt['path']['visualization'] = opt.get('visualization_path', os.path.join(opt['path']['root'], 'visualization'))\n",
    "        \n",
    "        \n",
    "        # change some options for debug mode\n",
    "        #debug enabled = True\n",
    "        if opt.get('debug', False):\n",
    "            opt['val_freq'] = 1\n",
    "            opt['print_freq'] = 1\n",
    "            opt['save_checkpoint_freq'] = 1\n",
    "\n",
    "    #for test mode    \n",
    "    else:  # test\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['results'] = opt.get('results_path', os.path.join(opt['path']['root'], 'results'))\n",
    "        opt['path']['log'] = opt.get('log_path', os.path.join(opt['path']['root'], 'test_logs'))\n",
    "        opt['path']['visualization'] = opt.get('vis_path', os.path.join(opt['path']['root'], 'test_visualizations'))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "import importlib\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to mask the image by turning everything outside the mask to white\n",
    "def mask_outside(img, seg_img):\n",
    "    \"\"\"\n",
    "    Mask everything outside the segmentation mask by turning non-masked pixels to white.\n",
    "    \"\"\"\n",
    "    img = np.array(img)  # Convert to numpy array\n",
    "    seg_img = np.array(seg_img)  # Convert segmentation mask to numpy array\n",
    "    \n",
    "    # Create a white image (255 means white)\n",
    "    white_img = np.ones_like(img) * 255\n",
    "\n",
    "    # Apply mask: keep the original image pixels where the mask is non-zero\n",
    "    masked_img = np.where(seg_img[:, :, None] != 0, img, white_img)\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))\n",
    "\n",
    "def compute_clip_score(img, seg_img, text_prompt):\n",
    "    \"\"\"\n",
    "    Compute the CLIP score for the image based on the segmentation mask where everything\n",
    "    outside the mask is set to white.\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image): The original image.\n",
    "        seg_img (PIL.Image): The segmentation mask.\n",
    "        text_prompt (str): The text prompt to compare the image to.\n",
    "\n",
    "    Returns:\n",
    "        float: The CLIP score for the masked image and text prompt.\n",
    "    \"\"\"\n",
    "    # Mask the image using the segmentation mask (keeping only the masked areas)\n",
    "    masked_img = mask_outside(img, seg_img)\n",
    "\n",
    "    # Preprocess the masked image\n",
    "    image = preprocess(masked_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode the image\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode the text prompt\n",
    "    text_input = clip.tokenize([text_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity (CLIP score)\n",
    "    score = (image_features @ text_features.T).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #parse arguments from dictionary\n",
    "    opt = parse(options_dict, is_train=True)\n",
    "    \n",
    "    \n",
    "    #load model cnbfiguration\n",
    "    model = create_model(opt) #create model from options (need create_model function from models module))\n",
    "    model.load_network()\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "\n",
    "    #load image\n",
    "    img = load_image(opt['img_path'])\n",
    "    #conver to tensor\n",
    "    img= torch.from_numpy(img).unsqueeze(dim=0).to(model.device)\n",
    "    \n",
    "    #load text inputs with clip to encode text\n",
    "    text_input = torch.cat([clip.tokenize(opt[\"text_prompt\"])]).to(model.device)\n",
    "\n",
    "    #pass image and text through encoder\n",
    "    with torch.no_grad():\n",
    "        # text embedding\n",
    "        text_embedding = model.clip.encode_text(text_input)\n",
    "        #encode text\n",
    "        text_encoding = model.encoder(img, text_embedding)\n",
    "        seg_logits = model.decoder(text_encoding)\n",
    "\n",
    "    #argmax to get segmentation map\n",
    "    seg_pred = seg_logits.argmax(dim=1).cpu().numpy()[0]\n",
    "    #convert to grayscale image\n",
    "    seg_img = Image.fromarray(np.uint8(seg_pred * 255))\n",
    "\n",
    "\n",
    "    # Save the generated mask\n",
    "\n",
    "    # Optionally, display the mask using matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(seg_img, cmap=\"gray\")\n",
    "    plt.title(\"Generated Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    img = Image.open(opt['img_path']).convert(\"RGB\").resize((256, 512))\n",
    "    \n",
    "    \n",
    "    # Load pipeline\n",
    "    #load stable diffusion inpaingting model (we can change this if we want)\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        opt['styleswap_model_path'], revision=\"fp16\",\n",
    "        torch_dtype=torch.float32, #16\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "        #move model to gpu\n",
    "    ).to(\"cpu\") #cuda\n",
    "\n",
    "    #generate image\n",
    "    generator = torch.Generator(\"cpu\").manual_seed(2023) #cuda\n",
    "    images = pipe(\n",
    "        height=512,\n",
    "        width=256,\n",
    "        prompt=[opt['text_prompt']],\n",
    "        image=img,\n",
    "        mask_image=seg_img,\n",
    "        num_inference_steps=50,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "    #blend generated image with original image\n",
    "    final_img = Image.composite(images[0], img, seg_img)\n",
    "    #save image\n",
    "    final_img.save(opt['output_path'])\n",
    "    \n",
    "    #compute clip score\n",
    "    clip_score = compute_clip_score(final_img, seg_img, opt['text_prompt'])\n",
    "    #print(f\"CLIP Score: {clip_score}\")\n",
    "    print(f\"CLIP Score: {clip_score}\")\n",
    "\n",
    "    print('Saved edited result to', opt['output_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAAGbCAYAAACI3n8JAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFdRJREFUeJzt3QmUjfUbwPFnjG0soVSiRJa0nYpCKoSQLaU6KUWLjEKpLGl1chIlHNLJcdosFdoURQsd5FTIHiJbdsa+jMy8//P86s5/5s4M95mY9515v59zbsvcd+793bn3e9/f+753ifM8zxMAMSkQ22IAFMEABgQDGBAMYEAwgAHBAAYEAxgQDGBAMIABweRjlSpVkk6dOkleGWurVq0k6AIZzLp166Rbt25SvXp1KVasmDtdeuml8thjj8mSJUskP5k2bZq89NJLvo4hLi7OnR5++OEsz3/22WfTltm1a5eEWeCC+eqrr+Tyyy+XsWPHSpMmTWTo0KEyfPhwueWWW9yD66qrrpINGzZIfqG3qX///n4PQ4oWLSqffPKJHDt2LNN5H374oTsfIgUlQNauXSt33323XHjhhfL999/Leeedl+H8QYMGyahRo6RAgcB1nubQoUNSvHhxyWuaN28uU6ZMka+//lpuvfXWtJ//9NNPbo3frl07F1TYBeqRN3jwYPeAe/fddzPFogoWLCg9evSQCy64IMPPV65cKXfccYeceeaZ7pnwmmuucXd+eu+9956bUsydO1eefPJJOfvss90D+7bbbpOdO3dmui594Nx4441umZIlS0rLli1l+fLlGZbR7YMSJUq40Fu0aOGWu/fee915s2fPljvvvFMqVqwoRYoUcWPu2bOnHDlyJMPvv/nmm+6/I1MePUWkpqbKsGHD5LLLLnO369xzz5UuXbrInj17MoxDX3A+YMAAOf/889309aabbso01pOpUKGC1K9fXyZMmJDh5+PHj5crrrjCrfWjxXIb1bZt2+SBBx5w49Pl9L7VKNevXy8n8v7777v7vFevXhIUBYM2HatatarUqVMn5t/RB8b111/v7vC+ffu6B/jEiROlbdu27hlRg0ive/fuUqZMGXnxxRfdHaYPSN1e+vjjj9OW0elgx44dpVmzZm6tdvjwYXnrrbfkhhtukN9++81toEYcP37cLafnvf766+4BqyZNmuR+r2vXrnLWWWfJL7/8IiNGjJC//vrLnaf0wb9lyxb59ttv3XVG0/M1dH2w6ROFPtOPHDnSjUHDL1SokFvuhRdecMFotHpauHChNG3aNMvp1Yncc8898vjjj8vBgwfdE4HeNh2rPsEcPXo00/Kx3Ealaye9n/Rvr3+7HTt2uNu8cePGDH/L9EaPHi2JiYnSr18/d9sCwwuIffv26ftyvLZt22Y6b8+ePd7OnTvTTocPH047r3Hjxt4VV1zhHT16NO1nqampXr169bxq1aql/ezdd991l9+kSRN3fkTPnj29+Ph4b+/eve7/Dxw44JUuXdrr3LlzhjFs27bNK1WqVIafd+zY0V1m3759M405/RgjBg4c6MXFxXkbNmxI+9ljjz3mLiPa7Nmz3c/Hjx+f4efffPNNhp/v2LHDK1y4sNeyZcsMt6tfv35uOR3jyehyOo6kpCR3WWPHjnU/nzp1qhvv+vXrvRdffNEtp39/y23U+05/77XXXjvhGC688EJ3G9Tw4cPdZbz88ste0ARmSrZ//373b31mi9awYUM3hYqcItOYpKQk+eGHH+Suu+6SAwcOuD04etq9e7d71v/jjz9k8+bNGS7rkUceyTDt0WlXSkpK2o4Efebbu3evtG/fPu3y9BQfH+/WfDNnzsw0Pn2GjZaQkJD23zrN1MuoV6+emz7pGuJk9Bm6VKlScvPNN2cYR61atdzfKDKO7777zq1J9Nk7/e164oknxErXvLotoxv5SqdnOmbdpsxKQgy3UZcpXLiwzJo1K9NUMrtpua7ldM3+3HPPSdAEZkqm83+l04Fob7/9tgti+/bt0qFDh7Sfr1mzxt05zz//vDtlRVf/Ol2L0Pl29INERe5MjUw1atQoy8s744wzMvy/zrF1bh5Npxs6VdJtqegHyr59++RkdBy63DnnnJPt7VKR0KtVq5bhfH1iidw267Tsvvvuc+P//PPP3QM4OxtjuI26zaIP/qeeesptg9WtW9cdb7n//vulXLlyGX7nxx9/lKlTp0qfPn0Ctd0SyGD02VQ3BpctW5bpvMg2TfRGom4Uq6efftqtUbKi20Tp6ZoiK5F3akcuU7cpou/QSCDp6QMieq+drrF0zaBrQL3za9So4batdG2nG/qR6zgRXUZj0Y3urGgQp0ObNm3cbdJtuOTkZLf2zkqK4Tbq2q5169YuwOnTp7snt4EDB7rZwdVXX522nO7c0LW7/u11+61y5coSNIEJRumeqDFjxriNx9q1a590+Ysuusj9Wzd+9ZjNqVClShX3b32w5vQyly5dKqtXr3Z7efSZNEKne9HST6Oix6HTLd2hkX7qEy0yXdI1UuTvoXTPXyxToGh6XbrDZNy4ce7YV9myZf/zbYzcHl3L6EnHqsfThgwZ4q4nQq9r8uTJbgdK48aNZc6cOVK+fHkJksBsw6jevXu7vUwPPvigm35Fi/68Dn1Q6/aNTtm2bt2aafmsdhefjK6pdNr1yiuvyN9//52jy4ysxdKPV/9bD8BGixyz0WfW9PSZXZ/FX3755Uy/o3uvIstr1PqEoXun0l+f7v3LKV1j617E7Ka5ltuoe9Gi97BpPDoF1zVYNJ3e6hOF7prWNZhujwZJoNYwOg/XDU3d4L744ovdMY0rr7zS3RG6S1XP0+lP+m0G3QGgz0h6rKBz587uWVZjmzdvntu9uXjxYtMYNBbdhazz+Jo1a7oDqTr90fm6zq/1GV937Z6ITk/0QaEPPJ2i6GXqLu6snvF1I17pbmONVR+Iep0NGjRw0xKduixatMjtJtYw9NlZdwjoA1OPPenY9Hp0Od020N3KusGtx5GyWzucjP7N9XQqbuPq1avd2kKfAPTlTTql/eyzz9x9pLczu2n0jBkz3JOh/k106ha97egbL4DWrFnjde3a1atatapXtGhRLyEhwatRo4aXmJjoLVq0KNPya9eu9e6//36vXLlyXqFChbwKFSp4rVq18iZPnpxpt/Kvv/6a4Xdnzpzpfq7/jv55s2bN3K5kHUOVKlW8Tp06efPnz09bRnfZFi9ePMvbsGLFCrcLu0SJEl7ZsmXd7ujFixe769KxRBw/ftzr3r27d/bZZ7tdqdF3yejRo71atWq5v0HJkiXdLvTevXt7W7ZsSVsmJSXF69+/v3feeee55Ro2bOgtW7bM7aq17FY+kax2K6+I4Tbu2rXLXbbef/q30r9nnTp1vIkTJ2a7Wzni559/dre5fv36We7C9kOc/sPvaIG8IlDbMEDQEQxgQDCAAcEABgQDGBAMcDoOXGb3Eg4gv4jlCAtrGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCAYwIBjAgGAAA4IBDAgGMCAYwIBgAAOCAQwIBjAgGMCAYAADggEMCloWRngULVpUChb85+FRunRpGTdunMTFxcmaNWuka9eucuzYMQklL0a6KKf8fypevLjXsmVLb968eV5ycnLaKTU11T0OUlJSvP79+/s+ztNxigVrGGQwcOBA6d69e7bnFyhQQOLj4yWs2IaBm2rde++9brqVmJh40uVLly4tRYoUkVBiShbuU61atbyePXt6x48fj/Wh4On0rF69er6P/VSfmJIh2zVKoUKF5J133pFatWpJjRo1zL8fVgQTMjVr1pQ2bdpIz549pWTJkjl68G/atEn2798vocSULDyn5s2bezt37vT+qwEDBvh+W07HKRasYUKgbNmy8uWXX8pFF13k/hs5x16yfEynXO3bt5dvv/1W6tSpI+ecc47fQ8rzWMPkUw8++KC0bt1a2rZt6/dQ8hWCOQ0qVKgg8+fPl5SUFGnXrp0cOnTohMtv3bpVdu/e/Z+vV9cgOu2aPHmylClTRooVKyan2pEjR2T79u0SWrFu6Pm9QZaXThUrVkx7KUkspkyZ4iUmJub4+uLi4rwePXp4M2bM8E63BQsW+P73PV2nWBDMaThVr17dFIw6cuSI16dPH69YsWIxX09CQoLXtm1bb+bMmd6xY8e83LAg5MHE6T9iWROF+WCV1fLly+XSSy81/55O4ZKTk6Vjx46ZpnEHDx6U2bNny7XXXpu2p2vkyJFywQUXuIOQuWXhwoXuYGd+FEsKbMOcBpGXxVvpixp1u2PSpEmZzktKSnIvsdeN+IoVK4ofUlNTZdSoURJmrGFOg1WrVkn16tUlv0lNTXU7Fk7FDoogYg1j0KhRI6lWrZr89ttv8ssvv/g9HAQUwYhI/fr15b333nPbA6tXr3bz9A4dOrhtCiA9gvn37bh6UjqV0nCuv/56F8+2bdtMl1W5cmVJSEiQ/MiLbfaer7EN86+xY8e6tUp633zzjfz666+Zlj18+LC8+uqrmY6sV6pUSZo1aya1a9eW/CiVbRjWMCfSvHlzd4p2/PhxueOOOzL8TNdM+tot5G8Ek8Pdxvn1WMSJeEzJeLWyqlu3rlx33XV+DyPwChQoIE8//bSEWeiD0YOFN954o1SpUsXvoQReXFycNG3aVMIs9MF06dJFBg0a5PcwkEeEPhj9aCHm5ohV6INZtGiR30NAHhL6YAALgoFJ4cKFpVSpUhJWBAOTyy67THr16iVhRTAwWbdunUyYMEHCimBgsnfvXlmxYoWEFcEABqEPhhdMwiL0wej75/P7Wxdw6oQ+GH1BIcEgVqEPBjaXX3659OnTR8Iq9MHoOy2HDBkS6j0/1gOXTZo0kfLly0soxfqJh35/KuHpPDVo0MBbu3at+dMqw2zWrFlefHy87/cdn3zp03ti9Jnz008/zfItychs//79ctZZZ7m3a+cXsaQQ+imZ0o9T0k+lD/PcHLEhmHT4di6cDMH8q2HDhvLZZ5/5PQwEHMH866GHHpIzzjjD72HkGSkh/VRQggkw3aDWt1AHkX53Zn7a4I8VwQR8T9Ttt9/uPus5aA4cOCBhRDB54MuZghbM0aNHmZKFnX6Gsj4QcHJDhw6Vn3/+WcKIYP41YsQI2bdvn9/DyBMH91JTUyWsCAYmy5Ytk8GDB0tYEQxM/v77b7czIqwIJt1Uo1+/fn4PI/B/o2PHjkmYEUw68+fP93sIgQ+mXbt2EmYEk87WrVtl6tSpEsS3UQdlh0RycrKEGcGks3PnTpkzZ44EzYwZM+TgwYN+D0P27NkT6j1kimCiLFmyxPxFsGHRrVs3F02YEUyUadOmye+//x64r8AYMGCA30MAwWTtnnvucbtPg2TWrFm+Xv+uXbskKSlJwo5gshDWFxaeiO4MmTFjhoQdwWRB3678zDPP+D0MBBDBZEH3BK1atcrvYQSGvih1wYIFfg8jEAgmG0uXLpWZM2f6PYzAfGK/vjgVBJOtjRs3uveiAOkRzAn8+OOPgTnCjmAgmBOYPHmy250adrwo9f8IBjG9GxX/IJiTGD16dOCO+sM/BHMSEydOlCDQjzQK8xu3goJg8gj9fLL+/fv7PYzQI5gYhHlKNn78eNm0aZPfwwgMgonheEzv3r0lrBYvXsyu9XQIJoaXyRw6dMjvYSAgCCYG+qn+QXwnJnIfwcRg+/btvBcEDsHkMWHeAREEBBOjrl27+v5BFGPGjJG5c+f6OoawIxjDJ8r4/YkpeuBS39wG/xAMYEAwAVawYEG55JJL/B4G0iGYANPv3HzhhRf8HgbSIRjAgGAAA4LJY954443Qf+WEnwgmj9FPwMzNL2TVbagaNWrk2vUFHcEEmB5z+eKLLzK9kUy//iK3lChRwu2twz8IJkb6QM3tl/lrMB999FGmcYwdOzZXx4H/IxjDa7j0C1ERbgSTxxQqVEgSExP9HkZoEUzAv0IwWnx8vLRo0cKX8YBgAq1Tp06ZXvCpsWg08AfB5DE6HStcuLDfwwgtgsljVqxYkavHYZARwcSoQIECborkt759+3Kk30cEEyPdbrjrrrty7freeuutbL9ug4898g/BxEg3vnPzi1n1q8+zenelfhtY06ZN3Tc9I/cRTIx0u2Hw4MESlG9H+/rrr/0eRigRDGBAMIABwQAGBBNQ1apVk1KlSvk9DEQhmIDq0KGDiwbBQjCAAcEABgSTB8XFxfGKZZ8QjMGBAwcC8fV1DRo0yLU3ka1fv14OHz6cK9eVFxCMwZIlS+SDDz7wexjuQymKFCmSK9c1atQo+fPPP3PluvICggEMCAYwIBjAgGAAA4Ixmj17tvuSWIQTwRhNnz5dNm/e7Pcw4BOCQbZ2797NLuUoBINsLV68WD755BO/hxEoBAMYEEwOTJs2zX04OcKHYHJg2LBhBBNSBINsP1Zq5cqVfg8jcAgGWdJP1+zVq5ffwwgcgskBnY4dPHjQ72HABwSTA0lJSfLoo4/6PQz4gGByKPp7W/KbkSNHSnJyst/DCByCySH9QPC9e/dKfvX999/ztRpZIJj/cCxmypQpkh8tXLhQ1q1b5/cwAolgkMmcOXNk1apVfg8jkOI8jsABMWMNAxgQDGBAMIABwQAGBAMYEAxgQDCAAcEABgQDSOz+B4re20Spj0trAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08fd5265819421fac61d9135774b177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e8e2d8f7ff4a0c935fe19a69cdba51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.29182392]\n",
      "Saved edited result to example_output2.png\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over text prompt for same image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompts = [\n",
    "    'blue upper clothing',\n",
    "    'purple shirt',\n",
    "    'blue denim lower clothing',\n",
    "    'white pants',\n",
    "    'floral shirt',\n",
    "    'grey cotton sweatpants'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text prompt: blue upper clothing\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d347564e54441dfa0afa541ee62cbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea756b9ddc418abe40765574c33107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.30590147]\n",
      "Saved edited result to ../blue_upper_clothing_output.png\n",
      "Processing text prompt: purple shirt\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35915f88df24d4cb91d3baef349507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eba0b4a54f44adfb274685e587cd631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.30381265]\n",
      "Saved edited result to ../purple_shirt_output.png\n",
      "Processing text prompt: blue denim lower clothing\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9896279ce8374efb9b866a0ffdbf0f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e43dd2e554db6be33207bd32b2095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.30811357]\n",
      "Saved edited result to ../blue_denim_lower_clothing_output.png\n",
      "Processing text prompt: white pants\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472df70c8a33449bb69085a7d32be2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f40bb417ed41b8acedacca1174cbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.30131274]\n",
      "Saved edited result to ../white_pants_output.png\n",
      "Processing text prompt: floral shirt\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ca6151d9249819d2b2d59d4b610d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8b3a2931fd401785feee9f2d63913c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.26972473]\n",
      "Saved edited result to ../floral_shirt_output.png\n",
      "Processing text prompt: grey cotton sweatpants\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4803f4afaf742ebae8cc0ab955f15ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f50582b1bc747268dd169deed912fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.26925653]\n",
      "Saved edited result to ../grey_cotton_sweatpants_output.png\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each text prompt\n",
    "for text_prompt in text_prompts:\n",
    "    # Update the text prompt in the options dictionary\n",
    "    options_dict[\"text_prompt\"] = text_prompt\n",
    "    options_dict[\"output_path\"] = f\"../{text_prompt.replace(' ', '_')}_output.png\"  # Save with a unique name\n",
    "\n",
    "    # Call the main function\n",
    "    print(f\"Processing text prompt: {text_prompt}\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29333333333333333"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clip score avg: \n",
    "sum = .31+.3+.31+.3+.27+.27\n",
    "total = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over multiple images and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '../test_images/'\n",
    "prompts = [\n",
    "    'red leather jacket',\n",
    "    'blue cotton t-shirt',\n",
    "    'tan jacket'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get all image paths from a folder\n",
    "def get_image_paths(folder):\n",
    "    \"\"\"\n",
    "    Get all image file paths from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        list: List of image file paths.\n",
    "    \"\"\"\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(valid_extensions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_with_prompts(image_folder, prompts):\n",
    "    \"\"\"\n",
    "    Process all images in a folder with a list of prompts.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        prompts (list): List of text prompts.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image_paths = get_image_paths(image_folder)\n",
    "    for image_path in image_paths:\n",
    "        for prompt in prompts:\n",
    "            # Update the options dictionary for each image and prompt\n",
    "            options_dict[\"img_path\"] = image_path\n",
    "            options_dict[\"text_prompt\"] = prompt\n",
    "            options_dict[\"output_path\"] = f\"../output/{os.path.basename(image_path).split('.')[0]}_{prompt.replace(' ', '_')}_output.png\"\n",
    "\n",
    "            # Ensure the output directory exists\n",
    "            os.makedirs(os.path.dirname(options_dict[\"output_path\"]), exist_ok=True)\n",
    "\n",
    "            # Call the main function\n",
    "            print(f\"Processing image: {image_path} with prompt: '{prompt}'\")\n",
    "            main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../test_images/WOMEN-Pants-id_00003608-10_1_front.png with prompt: 'red leather jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c2d9b236984822bb8b1a3d7e220d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dbe13fa7cf40b5add8c5989f84cb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.2747423]\n",
      "Saved edited result to ../output/WOMEN-Pants-id_00003608-10_1_front_red_leather_jacket_output.png\n",
      "Processing image: ../test_images/WOMEN-Pants-id_00003608-10_1_front.png with prompt: 'blue cotton t-shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ca2ec2fe02455097e645473887cda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263b66f3f58d40ebbae0d76e1204303a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.25690812]\n",
      "Saved edited result to ../output/WOMEN-Pants-id_00003608-10_1_front_blue_cotton_t-shirt_output.png\n",
      "Processing image: ../test_images/WOMEN-Pants-id_00003608-10_1_front.png with prompt: 'tan jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2422e74cc6764f309128292cf7876782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f203c8d8f307419891e0464489870081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.25015134]\n",
      "Saved edited result to ../output/WOMEN-Pants-id_00003608-10_1_front_tan_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Denim-id_00005208-01_7_additional.png with prompt: 'red leather jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bedaa9c78b4440cae3e73a10e440b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b93cb34ece4169a2a2996a68bc392e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.28986913]\n",
      "Saved edited result to ../output/MEN-Denim-id_00005208-01_7_additional_red_leather_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Denim-id_00005208-01_7_additional.png with prompt: 'blue cotton t-shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa564caf5b406084c60130665485e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e855255fbd94d79bc75d692879ecc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.25226337]\n",
      "Saved edited result to ../output/MEN-Denim-id_00005208-01_7_additional_blue_cotton_t-shirt_output.png\n",
      "Processing image: ../test_images/MEN-Denim-id_00005208-01_7_additional.png with prompt: 'tan jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eacf691d3345d9808124168ad2bbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99912936d2e7425ea32fad0ac17b0453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.24363872]\n",
      "Saved edited result to ../output/MEN-Denim-id_00005208-01_7_additional_tan_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Jackets_Vests-id_00004319-03_4_full.png with prompt: 'red leather jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3258c736bafd4b559c0597ac2b1227c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0535a0ba4bbe4b52919e83499f5a5e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.28157932]\n",
      "Saved edited result to ../output/MEN-Jackets_Vests-id_00004319-03_4_full_red_leather_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Jackets_Vests-id_00004319-03_4_full.png with prompt: 'blue cotton t-shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfcee17d85f457f99f1862c780552ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6e35a508944c4ab67b32518b849aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.26868492]\n",
      "Saved edited result to ../output/MEN-Jackets_Vests-id_00004319-03_4_full_blue_cotton_t-shirt_output.png\n",
      "Processing image: ../test_images/MEN-Jackets_Vests-id_00004319-03_4_full.png with prompt: 'tan jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027e3c8e8a5246c98225370635b4c71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_10000/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_10000/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360e15963c0d4dbe8f741628e152dea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.24453641]\n",
      "Saved edited result to ../output/MEN-Jackets_Vests-id_00004319-03_4_full_tan_jacket_output.png\n"
     ]
    }
   ],
   "source": [
    "# Call the function to process all images with prompts\n",
    "process_images_with_prompts(image_folder, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over images, one prompt per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_with_one_prompt(image_prompt_pairs, output_folder):\n",
    "    \"\"\"\n",
    "    Process each image with its corresponding prompt.\n",
    "\n",
    "    Args:\n",
    "        image_prompt_pairs (list): A list of tuples where each tuple contains:\n",
    "                                   - image_path (str): Path to the image.\n",
    "                                   - prompt (str): The text prompt for the image.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for image_path, prompt in image_prompt_pairs:\n",
    "        # Update the options dictionary for the current image and prompt\n",
    "        options_dict[\"img_path\"] = image_path\n",
    "        options_dict[\"text_prompt\"] = prompt\n",
    "        options_dict[\"output_path\"] = f\"../{output_folder}/{os.path.basename(image_path).split('.')[0]}_{prompt.replace(' ', '_')}_output.png\"\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(options_dict[\"output_path\"]), exist_ok=True)\n",
    "\n",
    "        # Call the main function\n",
    "        print(f\"Processing image: {image_path} with prompt: '{prompt}'\")\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run through test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../test_images/MEN-Denim-id_00005208-01_7_additional.png with prompt: 'dark red leather jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360539327c5c4fa79e3f660a978898e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb7ed6fd52549deb5744abc255000a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.29560393]\n",
      "Saved edited result to ../output/MEN-Denim-id_00005208-01_7_additional_dark_red_leather_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Jackets_Vests-id_00004319-03_4_full.png with prompt: 'blue cotton t-shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ca087167474052bf0d0aac287ad74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af644472c237425e9afa2b0e8a16ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.25876188]\n",
      "Saved edited result to ../output/MEN-Jackets_Vests-id_00004319-03_4_full_blue_cotton_t-shirt_output.png\n",
      "Processing image: ../test_images/MEN-Shorts-id_00000989-01_1_front.png with prompt: 'green shorts'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cded33a70c49caa25772cbf00ea65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b76f4e1de445888a74a2a18d475857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.29091728]\n",
      "Saved edited result to ../output/MEN-Shorts-id_00000989-01_1_front_green_shorts_output.png\n",
      "Processing image: ../test_images/WOMEN-Blouses_Shirts-id_00000108-03_7_additional.png with prompt: 'gray denim pants'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe2ae4ea9a74f088ee060ecb3cfcf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b592f8616ec247a996bdcc4c16619ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.30151582]\n",
      "Saved edited result to ../output/WOMEN-Blouses_Shirts-id_00000108-03_7_additional_gray_denim_pants_output.png\n",
      "Processing image: ../test_images/WOMEN-Blouses_Shirts-id_00003914-05_4_full.png with prompt: 'purple upper clothing'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a575654dca94438194bfb758c08b8ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1533f95ad1634f8eab9beaf7c231b27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.28155595]\n",
      "Saved edited result to ../output/WOMEN-Blouses_Shirts-id_00003914-05_4_full_purple_upper_clothing_output.png\n",
      "Processing image: ../test_images/WOMEN-Dresses-id_00002326-04_4_full.png with prompt: 'white dress'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ef9bde85ff4204b6b63c1bbb7bff90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5758a98950463086500a0200298874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.26985812]\n",
      "Saved edited result to ../output/WOMEN-Dresses-id_00002326-04_4_full_white_dress_output.png\n",
      "Processing image: ../test_images/WOMEN-Pants-id_00003608-10_1_front.png with prompt: 'striped shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14063c7bdc2d4a69a95993984b79a43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8dc2ab0d743c0b4ca37c60ec14f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.24639778]\n",
      "Saved edited result to ../output/WOMEN-Pants-id_00003608-10_1_front_striped_shirt_output.png\n"
     ]
    }
   ],
   "source": [
    "# folder and prompts\n",
    "image_folder = \"../test_images/\"\n",
    "prompts = [\n",
    "    \"dark red leather jacket\",\n",
    "    \"blue cotton t-shirt\",\n",
    "    'green shorts',\n",
    "    \"gray denim pants\",\n",
    "    'purple upper clothing',\n",
    "    'white dress',\n",
    "    'striped shirt'\n",
    "]\n",
    "\n",
    "# Get all image paths in abc order\n",
    "def get_image_paths(folder):\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "    return sorted(\n",
    "        [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(valid_extensions)]\n",
    "    )\n",
    "image_paths = get_image_paths(image_folder)\n",
    "\n",
    "# Pair each image with a prompt (ensure the lengths match)\n",
    "image_prompt_pairs = list(zip(image_paths, prompts))\n",
    "\n",
    "#output folder\n",
    "output = 'output'\n",
    "\n",
    "# Process each image with its corresponding prompt\n",
    "process_images_with_one_prompt(image_prompt_pairs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2785714285714286"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clip avg\n",
    "sum2 = .3 +.27+.29+.3+.28+.27 +.24\n",
    "total2 = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment with vauge descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../test_images/MEN-Denim-id_00005208-01_7_additional.png with prompt: 'stylish jacket'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4d076396254301873909f0f8ebcc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2586d3c991411f8f37d6b7fa29d48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.24936458]\n",
      "Saved edited result to ../output_vauge/MEN-Denim-id_00005208-01_7_additional_stylish_jacket_output.png\n",
      "Processing image: ../test_images/MEN-Jackets_Vests-id_00004319-03_4_full.png with prompt: 'cute t-shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de340556264f27b7692fc7caf09af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed61f958ccdd4f3b814ce63b110e966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.22864878]\n",
      "Saved edited result to ../output_vauge/MEN-Jackets_Vests-id_00004319-03_4_full_cute_t-shirt_output.png\n",
      "Processing image: ../test_images/MEN-Shorts-id_00000989-01_1_front.png with prompt: 'fun shorts'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da9e9610ab8425295a4928bc875c5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d956b8c1d8847cbb51f8500dd8dbf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.27801916]\n",
      "Saved edited result to ../output_vauge/MEN-Shorts-id_00000989-01_1_front_fun_shorts_output.png\n",
      "Processing image: ../test_images/WOMEN-Blouses_Shirts-id_00000108-03_7_additional.png with prompt: 'dark lower clothing'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563768583ad844968e967a0b0ec5bd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb9f141da34c9ba362fdf8820e1cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.27214226]\n",
      "Saved edited result to ../output_vauge/WOMEN-Blouses_Shirts-id_00000108-03_7_additional_dark_lower_clothing_output.png\n",
      "Processing image: ../test_images/WOMEN-Blouses_Shirts-id_00003914-05_4_full.png with prompt: 'stylish upper clothing'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e83f4aebf9b4af58c775ce1fc477d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f183ae41934c579f8c0195549cf1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.25115824]\n",
      "Saved edited result to ../output_vauge/WOMEN-Blouses_Shirts-id_00003914-05_4_full_stylish_upper_clothing_output.png\n",
      "Processing image: ../test_images/WOMEN-Dresses-id_00002326-04_4_full.png with prompt: 'light dress'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9470018cc3aa433a841d4dce287b9222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e47c22e426490b9aa86cfc05efb2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.2876626]\n",
      "Saved edited result to ../output_vauge/WOMEN-Dresses-id_00002326-04_4_full_light_dress_output.png\n",
      "Processing image: ../test_images/WOMEN-Pants-id_00003608-10_1_front.png with prompt: 'crazy shirt'\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca243eb4d9f2497e80e0136d4c0b925c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../fashion-model-finetuned_full/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../fashion-model-finetuned_full/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5783d8e1074418985bc6dd5f1ba830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: [0.21370217]\n",
      "Saved edited result to ../output_vauge/WOMEN-Pants-id_00003608-10_1_front_crazy_shirt_output.png\n"
     ]
    }
   ],
   "source": [
    "# folder and prompts\n",
    "image_folder = \"../test_images/\"\n",
    "prompts = [\n",
    "    \"stylish jacket\",\n",
    "    \"cute t-shirt\",\n",
    "    'fun shorts',\n",
    "    \"dark lower clothing\",\n",
    "    'stylish upper clothing',\n",
    "    'light dress',\n",
    "    'crazy shirt'\n",
    "]\n",
    "\n",
    "# Get all image paths in abc order\n",
    "def get_image_paths(folder):\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "    return sorted(\n",
    "        [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(valid_extensions)]\n",
    "    )\n",
    "image_paths = get_image_paths(image_folder)\n",
    "\n",
    "# Pair each image with a prompt (ensure the lengths match)\n",
    "image_prompt_pairs = list(zip(image_paths, prompts))\n",
    "\n",
    "#output folder\n",
    "output_vauge = 'output_vauge'\n",
    "\n",
    "# Process each image with its corresponding prompt\n",
    "process_images_with_one_prompt(image_prompt_pairs, output_vauge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip avg\n",
    "sum3 = .25+.23+.28+.27+.25+.29+.21\n",
    "total3 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2745"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total clip avg from experiments\n",
    "(sum+sum2+sum3)/ (total+total2+total3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 76.09522089627106\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Preprocessing for InceptionV3\n",
    "def preprocess_image(image, image_size=299):\n",
    "    transform = Compose([\n",
    "        Resize((image_size, image_size)),\n",
    "        CenterCrop(image_size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Extract features using InceptionV3\n",
    "def get_features(images, model, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for img_path in images:\n",
    "            # Open the image file\n",
    "            img = Image.open(img_path).convert(\"RGB\")  # Ensure it's in RGB format\n",
    "            img = preprocess_image(img).to(device)\n",
    "            feature = model(img)[0].cpu().numpy()\n",
    "            features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# Calculate FID\n",
    "def calculate_fid(real_features, generated_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Compute mean and covariance differences\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = sqrtm(sigma1 @ sigma2, disp=False)\n",
    "\n",
    "    # Numerical stability\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load InceptionV3 model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception.fc = torch.nn.Identity()  # Remove the classification head\n",
    "\n",
    "    # Load real and generated image\n",
    "    real_images = get_image_paths(image_folder)\n",
    "    generated_images = get_image_paths('../output')\n",
    "\n",
    "    # Extract features\n",
    "    real_features = get_features(real_images, inception, device)\n",
    "    generated_features = get_features(generated_images, inception, device)\n",
    "\n",
    "    # Calculate FID\n",
    "    fid_score = calculate_fid(real_features, generated_features)\n",
    "    print(f\"FID Score: {fid_score}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 1024\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('../test_images/WOMEN-Dresses-id_00002326-04_4_full.png')\n",
    "width, height = img.size\n",
    "print(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980 1961\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('../ash.jpg')\n",
    "width, height = img.size\n",
    "print(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_im = img.resize((512,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_im.save('../resized_ash.png', format= 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
