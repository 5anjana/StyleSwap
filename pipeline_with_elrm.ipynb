{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target, topk=1, thresh=None):\n",
    "    \"\"\"Calculate accuracy according to the prediction and target.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The model prediction, shape (N, num_class, ...)\n",
    "        target (torch.Tensor): The target of each prediction, shape (N, , ...)\n",
    "        topk (int | tuple[int], optional): If the predictions in ``topk``\n",
    "            matches the target, the predictions will be regarded as\n",
    "            correct ones. Defaults to 1.\n",
    "        thresh (float, optional): If not None, predictions with scores under\n",
    "            this threshold are considered incorrect. Default to None.\n",
    "\n",
    "    Returns:\n",
    "        float | tuple[float]: If the input ``topk`` is a single integer,\n",
    "            the function will return a single float as accuracy. If\n",
    "            ``topk`` is a tuple containing multiple integers, the\n",
    "            function will return a tuple containing accuracies of\n",
    "            each ``topk`` number.\n",
    "    \"\"\"\n",
    "    assert isinstance(topk, (int, tuple)) # topk should be int or tuple \n",
    "    if isinstance(topk, int):\n",
    "        topk = (topk, )\n",
    "        return_single = True\n",
    "    else:\n",
    "        return_single = False # convert topk to tuple if int, track how many values user passed\n",
    "\n",
    "    maxk = max(topk) # max number of top predictions we'll evaluate\n",
    "    if pred.size(0) == 0:\n",
    "        accu = [pred.new_tensor(0.) for i in range(len(topk))]\n",
    "        return accu[0] if return_single else accu # check if pred batch is empty\n",
    "    assert pred.ndim == target.ndim + 1 # checks that pred has one more dimension than target\n",
    "    assert pred.size(0) == target.size(0) # same size\n",
    "    assert maxk <= pred.size(1), \\\n",
    "        f'maxk {maxk} exceeds pred dimension {pred.size(1)}'\n",
    "    pred_value, pred_label = pred.topk(maxk, dim=1) # selects topk predictions and their indices\n",
    "    # transpose to shape (maxk, N, ...)\n",
    "    pred_label = pred_label.transpose(0, 1)\n",
    "    correct = pred_label.eq(target.unsqueeze(0).expand_as(pred_label)) # makes correct a boolean matrix (whether top-k predictions match the target)\n",
    "    if thresh is not None:\n",
    "        # Only prediction values larger than thresh are counted as correct\n",
    "        correct = correct & (pred_value > thresh).t() # masks out prediction below threshold with top-k scores\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / target.numel()))\n",
    "    return res[0] if return_single else res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_loss(loss, reduction):\n",
    "    \"\"\"Reduce loss as specified.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Elementwise loss tensor.\n",
    "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
    "\n",
    "    Return:\n",
    "        Tensor: Reduced loss tensor.\n",
    "    \"\"\"\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch.\n",
    "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        assert weight.dim() == loss.dim()\n",
    "        if weight.dim() > 1:\n",
    "            assert weight.size(1) == 1 or weight.size(1) == loss.size(1)\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != 'none':\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred,\n",
    "                  label,\n",
    "                  weight=None,\n",
    "                  class_weight=None,\n",
    "                  reduction='mean',\n",
    "                  avg_factor=None,\n",
    "                  ignore_index=-100):\n",
    "    \"\"\"The wrapper function for :func:`F.cross_entropy`\"\"\"\n",
    "    # class_weight is a manual rescaling weight given to each class.\n",
    "    # If given, has to be a Tensor of size C element-wise losses\n",
    "    loss = F.cross_entropy(\n",
    "        pred,\n",
    "        label,\n",
    "        weight=class_weight,\n",
    "        reduction='none',\n",
    "        ignore_index=ignore_index)\n",
    "\n",
    "    # apply weights and do the reduction\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "    loss = weight_reduce_loss(\n",
    "        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_onehot_labels(labels, label_weights, target_shape, ignore_index):\n",
    "    \"\"\"Expand onehot labels to match the size of prediction.\"\"\"\n",
    "    bin_labels = labels.new_zeros(target_shape)\n",
    "    valid_mask = (labels >= 0) & (labels != ignore_index)\n",
    "    inds = torch.nonzero(valid_mask, as_tuple=True)\n",
    "\n",
    "    if inds[0].numel() > 0:\n",
    "        if labels.dim() == 3:\n",
    "            bin_labels[inds[0], labels[valid_mask], inds[1], inds[2]] = 1\n",
    "        else:\n",
    "            bin_labels[inds[0], labels[valid_mask]] = 1\n",
    "\n",
    "    valid_mask = valid_mask.unsqueeze(1).expand(target_shape).float()\n",
    "    if label_weights is None:\n",
    "        bin_label_weights = valid_mask\n",
    "    else:\n",
    "        bin_label_weights = label_weights.unsqueeze(1).expand(target_shape)\n",
    "        bin_label_weights *= valid_mask\n",
    "\n",
    "    return bin_labels, bin_label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred,\n",
    "                         label,\n",
    "                         weight=None,\n",
    "                         reduction='mean',\n",
    "                         avg_factor=None,\n",
    "                         class_weight=None,\n",
    "                         ignore_index=255):\n",
    "    \"\"\"Calculate the binary CrossEntropy loss.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, 1).\n",
    "        label (torch.Tensor): The learning label of the prediction.\n",
    "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "        reduction (str, optional): The method used to reduce the loss.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (list[float], optional): The weight for each class.\n",
    "        ignore_index (int | None): The label index to be ignored. Default: 255\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    \"\"\"\n",
    "    if pred.dim() != label.dim():\n",
    "        assert (pred.dim() == 2 and label.dim() == 1) or (\n",
    "                pred.dim() == 4 and label.dim() == 3), \\\n",
    "            'Only pred shape [N, C], label shape [N] or pred shape [N, C, ' \\\n",
    "            'H, W], label shape [N, H, W] are supported'\n",
    "        label, weight = _expand_onehot_labels(label, weight, pred.shape,\n",
    "                                              ignore_index)\n",
    "\n",
    "    # weighted element-wise losses\n",
    "    if weight is not None:\n",
    "        weight = weight.float()\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, label.float(), pos_weight=class_weight, reduction='none')\n",
    "    # do the reduction for the weighted loss\n",
    "    loss = weight_reduce_loss(\n",
    "        loss, weight, reduction=reduction, avg_factor=avg_factor)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_cross_entropy(pred,\n",
    "                       target,\n",
    "                       label,\n",
    "                       reduction='mean',\n",
    "                       avg_factor=None,\n",
    "                       class_weight=None,\n",
    "                       ignore_index=None):\n",
    "    \"\"\"Calculate the CrossEntropy loss for masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n",
    "            of classes.\n",
    "        target (torch.Tensor): The learning label of the prediction.\n",
    "        label (torch.Tensor): ``label`` indicates the class label of the mask'\n",
    "            corresponding object. This will be used to select the mask in the\n",
    "            of the class which the object belongs to when the mask prediction\n",
    "            if not class-agnostic.\n",
    "        reduction (str, optional): The method used to reduce the loss.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "        class_weight (list[float], optional): The weight for each class.\n",
    "        ignore_index (None): Placeholder, to be consistent with other loss.\n",
    "            Default: None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated loss\n",
    "    \"\"\"\n",
    "    assert ignore_index is None, 'BCE loss does not support ignore_index'\n",
    "    # TODO: handle these two reserved arguments\n",
    "    assert reduction == 'mean' and avg_factor is None\n",
    "    num_rois = pred.size()[0]\n",
    "    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n",
    "    pred_slice = pred[inds, label].squeeze(1)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        pred_slice, target, weight=class_weight, reduction='mean')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    \"\"\"CrossEntropyLoss.\n",
    "\n",
    "    Args:\n",
    "        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n",
    "            of softmax. Defaults to False.\n",
    "        use_mask (bool, optional): Whether to use mask cross entropy loss.\n",
    "            Defaults to False.\n",
    "        reduction (str, optional): . Defaults to 'mean'.\n",
    "            Options are \"none\", \"mean\" and \"sum\".\n",
    "        class_weight (list[float], optional): Weight of each class.\n",
    "            Defaults to None.\n",
    "        loss_weight (float, optional): Weight of the loss. Defaults to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 use_sigmoid=False,\n",
    "                 use_mask=False,\n",
    "                 reduction='mean',\n",
    "                 class_weight=None,\n",
    "                 loss_weight=1.0):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        assert (use_sigmoid is False) or (use_mask is False)\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.use_mask = use_mask\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "        if self.use_sigmoid:\n",
    "            self.cls_criterion = binary_cross_entropy\n",
    "        elif self.use_mask:\n",
    "            self.cls_criterion = mask_cross_entropy\n",
    "        else:\n",
    "            self.cls_criterion = cross_entropy\n",
    "\n",
    "    def forward(self,\n",
    "                cls_score,\n",
    "                label,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None,\n",
    "                **kwargs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        if self.class_weight is not None:\n",
    "            class_weight = cls_score.new_tensor(self.class_weight)\n",
    "        else:\n",
    "            class_weight = None\n",
    "        loss_cls = self.loss_weight * self.cls_criterion(\n",
    "            cls_score,\n",
    "            label,\n",
    "            weight,\n",
    "            class_weight=class_weight,\n",
    "            reduction=reduction,\n",
    "            avg_factor=avg_factor,\n",
    "            **kwargs)\n",
    "        return loss_cls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def compute_cross_entropy_loss(cls_score,\n",
    "                               label,\n",
    "                               *,\n",
    "                               use_sigmoid=False,\n",
    "                               use_mask=False,\n",
    "                               class_weight=None,\n",
    "                               loss_weight=1.0,\n",
    "                               weight=None,\n",
    "                               avg_factor=None,\n",
    "                               reduction='mean',\n",
    "                               reduction_override=None,\n",
    "                               ignore_index=-100,\n",
    "                               **kwargs):\n",
    "    \"\"\"\n",
    "    Functional equivalent of CrossEntropyLoss class forward method.\n",
    "\n",
    "    Args:\n",
    "        cls_score (Tensor): Prediction logits.\n",
    "        label (Tensor): Ground-truth labels.\n",
    "        use_sigmoid (bool): Use sigmoid + BCE.\n",
    "        use_mask (bool): Use mask cross-entropy.\n",
    "        class_weight (list[float] | Tensor | None): Per-class weight.\n",
    "        loss_weight (float): Scalar multiplier on the loss.\n",
    "        weight (Tensor | None): Sample-wise weighting.\n",
    "        avg_factor (float | None): Averaging factor for mean reduction.\n",
    "        reduction (str): 'none' | 'mean' | 'sum'.\n",
    "        reduction_override (str | None): Overrides the reduction method.\n",
    "        ignore_index (int): Label index to ignore.\n",
    "        kwargs: Passed to the specific loss function.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "    \"\"\"\n",
    "    assert not (use_sigmoid and use_mask), \\\n",
    "        \"Cannot use both sigmoid and mask mode.\"\n",
    "\n",
    "    reduction = reduction_override if reduction_override else reduction\n",
    "\n",
    "    # Select the appropriate core loss function\n",
    "    if use_sigmoid:\n",
    "        loss_fn = binary_cross_entropy\n",
    "    elif use_mask:\n",
    "        loss_fn = mask_cross_entropy\n",
    "    else:\n",
    "        loss_fn = cross_entropy\n",
    "\n",
    "    # Convert class weights to tensor if needed\n",
    "    if class_weight is not None and not torch.is_tensor(class_weight):\n",
    "        class_weight = cls_score.new_tensor(class_weight)\n",
    "\n",
    "    return loss_weight * loss_fn(\n",
    "        cls_score,\n",
    "        label,\n",
    "        weight=weight,\n",
    "        class_weight=class_weight,\n",
    "        reduction=reduction,\n",
    "        avg_factor=avg_factor,\n",
    "        ignore_index=ignore_index,\n",
    "        **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet arch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import (ConvModule, build_activation_layer,\n",
    "                      build_norm_layer, build_upsample_layer)\n",
    "# from mmcv.runner import load_checkpoint\n",
    "# from mmcv.utils.parrots_wrapper import _BatchNorm\n",
    "# from mmseg.utils import get_root_logger\n",
    "# UPSAMPLE_LAYERS, constant_init, kaiming_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_class\n",
    "import torch\n",
    "from mmcv.cnn import ConvModule, build_upsample_layer\n",
    "\n",
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        conv_block,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        num_convs=2,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        with_cp=False,\n",
    "        conv_cfg=None,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        upsample_cfg=dict(type='InterpConv'),\n",
    "        dcn=None,\n",
    "        plugins=None\n",
    "        ):\n",
    "        \"\"\"Builds the upsample and conv blocks used in UNet decoder.\"\"\"\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "\n",
    "        self.conv_block = conv_block(\n",
    "            in_channels=2 * skip_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_convs=num_convs,\n",
    "            stride=stride,\n",
    "            dilation=dilation,\n",
    "            with_cp=with_cp,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg,\n",
    "            dcn=None,\n",
    "            plugins=None\n",
    "        )\n",
    "\n",
    "        if upsample_cfg is not None:\n",
    "            print(f\"Building UpConvBlock with upsample_cfg: {upsample_cfg}\")\n",
    "\n",
    "            self.upsample = build_upsample_layer(\n",
    "                cfg=upsample_cfg,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=skip_channels,\n",
    "                with_cp=with_cp,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "            print(f\"Upsample layer built: {self.upsample}\")\n",
    "        else:\n",
    "            self.upsample = ConvModule(\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, skip, x):\n",
    "        \"\"\"Forward function for upsample + conv block.\"\"\"\n",
    "        x = self.upsample(x)\n",
    "        out = torch.cat([skip, x], dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import ConvModule\n",
    "\n",
    "\n",
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_convs=2,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        with_cp=False,\n",
    "        conv_cfg=None,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        dcn=None,\n",
    "        plugins=None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Builds a basic convolutional block for UNet.\n",
    "\n",
    "        This block consists of several plain convolutional layers (Conv + Norm + Activation).\n",
    "\n",
    "        Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        num_convs (int): Number of convolutional layers. Default: 2.\n",
    "        stride (int): If stride=2, applies stride convolution in the first layer. Default: 1.\n",
    "        dilation (int): Dilation rate for all conv layers except the first. Default: 1.\n",
    "        with_cp (bool): If True, enables checkpointing for memory savings. Default: False.\n",
    "        conv_cfg (dict | None): Configuration for convolution layer. Default: None.\n",
    "        norm_cfg (dict | None): Configuration for normalization layer. Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Configuration for activation function. Default: dict(type='ReLU').\n",
    "        dcn (bool): Deformable convolution support. Not implemented. Default: None.\n",
    "        plugins (dict): Plugins for conv layers. Not implemented. Default: None.\n",
    "\n",
    "        Returns:\n",
    "        nn.Sequential: A sequential module containing the convolutional layers.\n",
    "        bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        \n",
    "        self.with_cp = with_cp\n",
    "        convs = []\n",
    "        for i in range(num_convs):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    in_channels=in_channels if i == 0 else out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=stride if i == 0 else 1,\n",
    "                    dilation=1 if i == 0 else dilation,\n",
    "                    padding=1 if i == 0 else dilation,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for basic convolutional block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            convs (nn.Sequential): Convolutional layers.\n",
    "            with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying convs.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.convs, x)\n",
    "        else:\n",
    "            out = self.convs(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import ConvModule\n",
    "from mmcv.cnn import UPSAMPLE_LAYERS\n",
    "\n",
    "@UPSAMPLE_LAYERS.register_module()\n",
    "class InterpConv(nn.Module):\n",
    "    \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "    This module uses interpolation to upsample feature map in the decoder\n",
    "    of UNet. It consists of one interpolation upsample layer and one\n",
    "    convolutional layer. It can be one interpolation upsample layer followed\n",
    "    by one convolutional layer (conv_first=False) or one convolutional layer\n",
    "    followed by one interpolation upsample layer (conv_first=True).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        conv_first (bool): Whether convolutional layer or interpolation\n",
    "            upsample layer first. Default: False. It means interpolation\n",
    "            upsample layer followed by one convolutional layer.\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "        stride (int): Stride of the convolutional layer. Default: 1.\n",
    "        padding (int): Padding of the convolutional layer. Default: 1.\n",
    "        upsampe_cfg (dict): Interpolation config of the upsample layer.\n",
    "            Default: dict(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 *,\n",
    "                 conv_cfg=None,\n",
    "                 conv_first=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 upsampe_cfg=dict(\n",
    "                     scale_factor=2, mode='bilinear', align_corners=False)):\n",
    "        super(InterpConv, self).__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "        conv = ConvModule(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg)\n",
    "        upsample = nn.Upsample(**upsampe_cfg)\n",
    "        if conv_first:\n",
    "            self.interp_upsample = nn.Sequential(conv, upsample)\n",
    "        else:\n",
    "            self.interp_upsample = nn.Sequential(upsample, conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            out = cp.checkpoint(self.interp_upsample, x)\n",
    "        else:\n",
    "            out = self.interp_upsample(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.cnn import UPSAMPLE_LAYERS, ConvModule\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "\n",
    "if 'InterpConv' not in UPSAMPLE_LAYERS:\n",
    "    @UPSAMPLE_LAYERS.register_module()\n",
    "    class InterpConv(nn.Module):\n",
    "        \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "        This module uses interpolation to upsample feature maps in the decoder\n",
    "        of UNet. It consists of one interpolation upsample layer and one\n",
    "        convolutional layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            with_cp (bool): Use checkpoint or not. Default: False.\n",
    "            norm_cfg (dict): Config dict for normalization layer. Default: dict(type='BN').\n",
    "            act_cfg (dict): Config dict for activation layer. Default: dict(type='ReLU').\n",
    "            conv_cfg (dict | None): Config dict for convolution layer. Default: None.\n",
    "            conv_first (bool): Whether convolutional layer or interpolation\n",
    "                upsample layer comes first. Default: False.\n",
    "            kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "            stride (int): Stride of the convolutional layer. Default: 1.\n",
    "            padding (int): Padding of the convolutional layer. Default: 0.\n",
    "            upsample_cfg (dict): Interpolation config of the upsample layer.\n",
    "                Default: dict(scale_factor=2, mode='bilinear', align_corners=False).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                     in_channels,\n",
    "                     out_channels,\n",
    "                     with_cp=False,\n",
    "                     norm_cfg=dict(type='BN'),\n",
    "                     act_cfg=dict(type='ReLU'),\n",
    "                     *,\n",
    "                     conv_cfg=None,\n",
    "                     conv_first=False,\n",
    "                     kernel_size=1,\n",
    "                     stride=1,\n",
    "                     padding=0,\n",
    "                     upsample_cfg=dict(scale_factor=2, mode='bilinear', align_corners=False)):\n",
    "            super().__init__()  # Correctly initialize the parent class\n",
    "\n",
    "            self.with_cp = with_cp\n",
    "            print(f\"Initializing InterpConv with in_channels={in_channels}, out_channels={out_channels}\")\n",
    "\n",
    "            # Convolutional layer\n",
    "            conv = ConvModule(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg\n",
    "            )\n",
    "\n",
    "            # Upsample layer\n",
    "            upsample = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "            # Sequential order\n",
    "            if conv_first:\n",
    "                self.interp_upsample = nn.Sequential(conv, upsample)\n",
    "            else:\n",
    "                self.interp_upsample = nn.Sequential(upsample, conv)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward function.\"\"\"\n",
    "            print(f\"InterpConv forward called with input shape: {x.shape}\")\n",
    "            if self.with_cp and x.requires_grad:\n",
    "                return cp.checkpoint(self.interp_upsample, x)\n",
    "            return self.interp_upsample(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "@UPSAMPLE_LAYERS.register_module()\n",
    "class InterpConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        with_cp=False,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        *,\n",
    "        conv_cfg=None,\n",
    "        conv_first=False,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        upsample_cfg=dict(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "    ):\n",
    "        super(InterpConv, self).__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "        \"\"\"\n",
    "        Builds an interpolation-based upsample module for the UNet decoder.\n",
    "\n",
    "        This module performs interpolation upsampling followed by a convolutional\n",
    "        block, or vice versa depending on `conv_first`.\n",
    "\n",
    "        Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpointing to reduce memory. Default: False.\n",
    "        norm_cfg (dict): Normalization layer config. Default: dict(type='BN').\n",
    "        act_cfg (dict): Activation layer config. Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Convolution config. Default: None.\n",
    "        conv_first (bool): Whether to apply convolution before upsampling. Default: False.\n",
    "        kernel_size (int): Kernel size of the convolution. Default: 1.\n",
    "        stride (int): Stride of the convolution. Default: 1.\n",
    "        padding (int): Padding for the convolution. Default: 0.\n",
    "        upsample_cfg (dict): Config for `nn.Upsample`. Default: bilinear 2x.\n",
    "\n",
    "        Returns:\n",
    "        nn.Sequential: A sequential module (Upsample + Conv or Conv + Upsample).\n",
    "        bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "        conv = ConvModule(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            conv_cfg=conv_cfg,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg\n",
    "        )\n",
    "\n",
    "        upsample = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        if conv_first:\n",
    "            self.module = nn.Sequential(conv, upsample)\n",
    "        else:\n",
    "            self.module = nn.Sequential(upsample, conv)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def interp_conv_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for the interpolation-based upsampling module.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        module (nn.Sequential): Upsample + Conv or Conv + Upsample.\n",
    "        with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor after upsampling and convolution.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.module, x)\n",
    "        else:\n",
    "            return self.module(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmcv.cnn import build_norm_layer, build_activation_layer\n",
    "\n",
    "\n",
    "class DeconvModule(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        with_cp=False,\n",
    "        norm_cfg=dict(type='BN'),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        *,\n",
    "        kernel_size=4,\n",
    "        scale_factor=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds a deconvolution upsample module for UNet decoder (2x upsample).\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            with_cp (bool): Whether to use checkpointing. Default: False.\n",
    "            norm_cfg (dict | None): Config dict for normalization layer. Default: dict(type='BN').\n",
    "            act_cfg (dict | None): Config dict for activation function. Default: dict(type='ReLU').\n",
    "            kernel_size (int): Kernel size of the transposed convolution. Default: 4.\n",
    "            scale_factor (int): Upsampling factor (stride). Default: 2.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential module for deconv -> norm -> activation.\n",
    "            bool: Whether checkpointing is enabled.\n",
    "        \"\"\"\n",
    "        assert (kernel_size - scale_factor >= 0) and \\\n",
    "            (kernel_size - scale_factor) % 2 == 0, (\n",
    "            f'Invalid kernel/scale config: kernel_size={kernel_size}, scale_factor={scale_factor}. '\n",
    "            'kernel_size must be >= scale_factor and their difference must be even.')\n",
    "\n",
    "        stride = scale_factor\n",
    "        padding = (kernel_size - scale_factor) // 2\n",
    "\n",
    "        deconv = nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "        _, norm = build_norm_layer(norm_cfg, out_channels)\n",
    "        activate = build_activation_layer(act_cfg)\n",
    "        \n",
    "\n",
    "        self.module = nn.Sequential(deconv, norm, activate)\n",
    "   \n",
    "\n",
    "\n",
    "    def deconv_module_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward function for the deconvolution upsampling module.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        module (nn.Sequential): Deconv -> Norm -> Activation module.\n",
    "        with_cp (bool): Whether checkpointing is enabled.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.module, x)\n",
    "        else:\n",
    "            return self.module(x)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "class InterpConv(nn.Module):\n",
    "    \"\"\"Interpolation upsample module in decoder for UNet.\n",
    "\n",
    "    This module uses interpolation to upsample feature map in the decoder\n",
    "    of UNet. It consists of one interpolation upsample layer and one\n",
    "    convolutional layer. It can be one interpolation upsample layer followed\n",
    "    by one convolutional layer (conv_first=False) or one convolutional layer\n",
    "    followed by one interpolation upsample layer (conv_first=True).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        conv_first (bool): Whether convolutional layer or interpolation\n",
    "            upsample layer first. Default: False. It means interpolation\n",
    "            upsample layer followed by one convolutional layer.\n",
    "        kernel_size (int): Kernel size of the convolutional layer. Default: 1.\n",
    "        stride (int): Stride of the convolutional layer. Default: 1.\n",
    "        padding (int): Padding of the convolutional layer. Default: 1.\n",
    "        upsampe_cfg (dict): Interpolation config of the upsample layer.\n",
    "            Default: dict(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 with_cp=False,\n",
    "                 norm_layer=nn.BatchNorm2d,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 conv_first=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 upsample_cfg=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        if upsample_cfg is None:\n",
    "            upsample_cfg=dict(scale_factor=2, model='bilinear', align_corners=False)\n",
    "        \n",
    "        layers = []\n",
    "        conv = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if norm_layer is not None:\n",
    "            conv.append(norm_layer(out_channels))\n",
    "        if act_layer is not None:\n",
    "            conv.append(act_layer())\n",
    "        \n",
    "        conv_block = nn.Sequential(*conv)\n",
    "        upsample_layer = nn.Upsample(**upsample_cfg)\n",
    "\n",
    "        if conv_first:\n",
    "            layers = [conv_block, upsample_layer]\n",
    "        else:\n",
    "            layers = [upsample_layer, conv_block]\n",
    "        \n",
    "        self.interp_upsample = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            return cp.checkpoint(self.interp_upsample, x)\n",
    "        return self.interp_upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet and atrunet\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet backbone.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondence encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondence decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondence encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convolution in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(UNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_outs = []\n",
    "\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            # logger = get_root_logger() library issue\n",
    "            # load_checkpoint(self, pretrained, strict=False, logger=logger) library issue\n",
    "\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            self.load_state_dict(state_dict, strcit=False)\n",
    "            print(f\"Loaded pretrained weights from {pretrained}\")\n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    # kaiming_init(m) library issue\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrUNet(nn.Module):\n",
    "    \"\"\"ShapeUNet backbone with small modifications.\n",
    "    U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input image channels. Default\" 3.\n",
    "        base_channels (int): Number of base channels of each stage.\n",
    "            The output channels of the first stage. Default: 64.\n",
    "        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n",
    "        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n",
    "            len(strides) is equal to num_stages. Normally the stride of the\n",
    "            first stage in encoder is 1. If strides[i]=2, it uses stride\n",
    "            convolution to downsample in the correspondance encoder stage.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance encoder stage.\n",
    "            Default: (2, 2, 2, 2, 2).\n",
    "        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n",
    "            convolution block of the correspondance decoder stage.\n",
    "            Default: (2, 2, 2, 2).\n",
    "        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n",
    "            feature map after the first stage of encoder\n",
    "            (stages: [1, num_stages)). If the correspondance encoder stage use\n",
    "            stride convolution (strides[i]=2), it will never use MaxPool to\n",
    "            downsample, even downsamples[i-1]=True.\n",
    "            Default: (True, True, True, True).\n",
    "        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n",
    "            Default: (1, 1, 1, 1, 1).\n",
    "        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n",
    "            Default: (1, 1, 1, 1).\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "            memory while slowing down the training speed. Default: False.\n",
    "        conv_cfg (dict | None): Config dict for convolution layer.\n",
    "            Default: None.\n",
    "        norm_cfg (dict | None): Config dict for normalization layer.\n",
    "            Default: dict(type='BN').\n",
    "        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n",
    "            Default: dict(type='ReLU').\n",
    "        upsample_cfg (dict): The upsample config of the upsample module in\n",
    "            decoder. Default: dict(type='InterpConv').\n",
    "        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n",
    "            freeze running stats (mean and var). Note: Effect on Batch Norm\n",
    "            and its variants only. Default: False.\n",
    "        dcn (bool): Use deformable convoluton in convolutional layer or not.\n",
    "            Default: None.\n",
    "        plugins (dict): plugins for convolutional layers. Default: None.\n",
    "\n",
    "    Notice:\n",
    "        The input image size should be devisible by the whole downsample rate\n",
    "        of the encoder. More detail of the whole downsample rate can be found\n",
    "        in UNet._check_input_devisible.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 num_stages=5,\n",
    "                 attr_embedding=128,\n",
    "                 strides=(1, 1, 1, 1, 1),\n",
    "                 enc_num_convs=(2, 2, 2, 2, 2),\n",
    "                 dec_num_convs=(2, 2, 2, 2),\n",
    "                 downsamples=(True, True, True, True),\n",
    "                 enc_dilations=(1, 1, 1, 1, 1),\n",
    "                 dec_dilations=(1, 1, 1, 1),\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 upsample_cfg=dict(type='InterpConv'),\n",
    "                 norm_eval=False,\n",
    "                 dcn=None,\n",
    "                 plugins=None):\n",
    "        super(AttrUNet, self).__init__()\n",
    "        assert dcn is None, 'Not implemented yet.'\n",
    "        assert plugins is None, 'Not implemented yet.'\n",
    "        assert len(strides) == num_stages, \\\n",
    "            'The length of strides should be equal to num_stages, '\\\n",
    "            f'while the strides is {strides}, the length of '\\\n",
    "            f'strides is {len(strides)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_num_convs) == num_stages, \\\n",
    "            'The length of enc_num_convs should be equal to num_stages, '\\\n",
    "            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n",
    "            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_num_convs) == (num_stages-1), \\\n",
    "            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n",
    "            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(downsamples) == (num_stages-1), \\\n",
    "            'The length of downsamples should be equal to (num_stages-1), '\\\n",
    "            f'while the downsamples is {downsamples}, the length of '\\\n",
    "            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(enc_dilations) == num_stages, \\\n",
    "            'The length of enc_dilations should be equal to num_stages, '\\\n",
    "            f'while the enc_dilations is {enc_dilations}, the length of '\\\n",
    "            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        assert len(dec_dilations) == (num_stages-1), \\\n",
    "            'The length of dec_dilations should be equal to (num_stages-1), '\\\n",
    "            f'while the dec_dilations is {dec_dilations}, the length of '\\\n",
    "            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n",
    "            f'{num_stages}.'\n",
    "        self.num_stages = num_stages\n",
    "        self.strides = strides\n",
    "        self.downsamples = downsamples\n",
    "        self.norm_eval = norm_eval\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            enc_conv_block = []\n",
    "            if i != 0:\n",
    "                if strides[i] == 1 and downsamples[i - 1]:\n",
    "                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n",
    "                upsample = (strides[i] != 1 or downsamples[i - 1])\n",
    "                self.decoder.append(\n",
    "                    UpConvBlock(\n",
    "                        conv_block=BasicConvBlock,\n",
    "                        in_channels=base_channels * 2**i,\n",
    "                        skip_channels=base_channels * 2**(i - 1),\n",
    "                        out_channels=base_channels * 2**(i - 1),\n",
    "                        num_convs=dec_num_convs[i - 1],\n",
    "                        stride=1,\n",
    "                        dilation=dec_dilations[i - 1],\n",
    "                        with_cp=with_cp,\n",
    "                        conv_cfg=conv_cfg,\n",
    "                        norm_cfg=norm_cfg,\n",
    "                        act_cfg=act_cfg,\n",
    "                        upsample_cfg=upsample_cfg if upsample else None,\n",
    "                        dcn=None,\n",
    "                        plugins=None))\n",
    "\n",
    "            enc_conv_block.append(\n",
    "                BasicConvBlock(\n",
    "                    in_channels=in_channels + attr_embedding,\n",
    "                    out_channels=base_channels * 2**i,\n",
    "                    num_convs=enc_num_convs[i],\n",
    "                    stride=strides[i],\n",
    "                    dilation=enc_dilations[i],\n",
    "                    with_cp=with_cp,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    dcn=None,\n",
    "                    plugins=None))\n",
    "            self.encoder.append((nn.Sequential(*enc_conv_block)))\n",
    "            in_channels = base_channels * 2**i\n",
    "\n",
    "    def forward(self, x, attr_embedding):\n",
    "        enc_outs = []\n",
    "        Be, Ce = attr_embedding.size()\n",
    "        for enc in self.encoder:\n",
    "            _, _, H, W = x.size()\n",
    "            x = enc(\n",
    "                torch.cat([\n",
    "                    x,\n",
    "                    attr_embedding.view(Be, Ce, 1, 1).expand((Be, Ce, H, W))\n",
    "                ],\n",
    "                          dim=1))\n",
    "            enc_outs.append(x)\n",
    "        dec_outs = [x]\n",
    "        for i in reversed(range(len(self.decoder))):\n",
    "            x = self.decoder[i](enc_outs[i], x)\n",
    "            dec_outs.append(x)\n",
    "\n",
    "        return dec_outs\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if isinstance(pretrained, str):\n",
    "            state_dict = torch.load(pretrained, map_location='cpu')\n",
    "            missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"Loaded pretrained wieghts from {pretrained}\")\n",
    "            if missing_keys:\n",
    "                print(f\"Missing keys: {missing_keys}\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "        \n",
    "        elif pretrained is None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm, nn.GroupNorm)):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError('pretrained must be a str or None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.cnn import ConvModule #, normal_init\n",
    "# from mmseg.ops import resize\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDecodeHead(nn.Module):\n",
    "    \"\"\"Base class for BaseDecodeHead.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int|Sequence[int]): Input channels.\n",
    "        channels (int): Channels after modules, before conv_seg.\n",
    "        num_classes (int): Number of classes.\n",
    "        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n",
    "        conv_cfg (dict|None): Config of conv layers. Default: None.\n",
    "        norm_cfg (dict|None): Config of norm layers. Default: None.\n",
    "        act_cfg (dict): Config of activation layers.\n",
    "            Default: dict(type='ReLU')\n",
    "        in_index (int|Sequence[int]): Input feature index. Default: -1\n",
    "        input_transform (str|None): Transformation type of input features.\n",
    "            Options: 'resize_concat', 'multiple_select', None.\n",
    "            'resize_concat': Multiple feature maps will be resize to the\n",
    "                same size as first one and than concat together.\n",
    "                Usually used in FCN head of HRNet.\n",
    "            'multiple_select': Multiple feature maps will be bundle into\n",
    "                a list and passed into decode head.\n",
    "            None: Only one select feature map is allowed.\n",
    "            Default: None.\n",
    "        loss_decode (dict): Config of decode loss.\n",
    "            Default: dict(type='CrossEntropyLoss').\n",
    "        ignore_index (int | None): The label index to be ignored. When using\n",
    "            masked BCE loss, ignore_index should be set to None. Default: 255\n",
    "        sampler (dict|None): The config of segmentation map sampler.\n",
    "            Default: None.\n",
    "        align_corners (bool): align_corners argument of F.interpolate.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False):\n",
    "        super(BaseDecodeHead, self).__init__()\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Extra repr.\"\"\"\n",
    "        s = f'input_transform={self.input_transform}, ' \\\n",
    "            f'ignore_index={self.ignore_index}, ' \\\n",
    "            f'align_corners={self.align_corners}'\n",
    "        return s\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        # normal_init(self.conv_seg, mean=0, std=0.01) don't have normal_init library\n",
    "\n",
    "        init.normal_(self.conv_seg.weight, mean=0, std=0.01)\n",
    "        if self.conv_seg.bias is not None:\n",
    "            init.constant_(self.conv_seg.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    input=x,\n",
    "                    size=inputs[0].shape[2:], # resizes to the spatial size of the first feature map\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Placeholder of forward function.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cls_seg(self, feat):\n",
    "        \"\"\"Classify each pixel.\"\"\"\n",
    "        if self.dropout is not None:\n",
    "            feat = self.dropout(feat)\n",
    "        output = self.conv_seg(feat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNHead(BaseDecodeHead):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 **kwargs):\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        super(FCNHead, self).__init__(**kwargs)\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ConvModule(\n",
    "                self.in_channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg))\n",
    "        for i in range(num_convs - 1):\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "        if num_convs == 0:\n",
    "            self.convs = nn.Identity()\n",
    "        else:\n",
    "            self.convs = nn.Sequential(*convs)\n",
    "        if self.concat_input:\n",
    "            self.conv_cat = ConvModule(\n",
    "                self.in_channels + self.channels,\n",
    "                self.channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                conv_cfg=self.conv_cfg,\n",
    "                norm_cfg=self.norm_cfg,\n",
    "                act_cfg=self.act_cfg)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "        output = self.convs(x)\n",
    "        if self.concat_input:\n",
    "            output = self.conv_cat(torch.cat([x, output], dim=1))\n",
    "        output = self.cls_seg(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadFCNHead(nn.Module):\n",
    "    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n",
    "\n",
    "    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n",
    "\n",
    "    Args:\n",
    "        num_convs (int): Number of convs in the head. Default: 2.\n",
    "        kernel_size (int): The kernel size for convs in the head. Default: 3.\n",
    "        concat_input (bool): Whether concat the input and output of convs\n",
    "            before classification layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 *,\n",
    "                 num_classes,\n",
    "                 dropout_ratio=0.1,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 act_cfg=dict(type='ReLU'),\n",
    "                 in_index=-1,\n",
    "                 input_transform=None,\n",
    "                 ignore_index=255,\n",
    "                 align_corners=False,\n",
    "                 num_convs=2,\n",
    "                 kernel_size=3,\n",
    "                 concat_input=True,\n",
    "                 num_head=18,\n",
    "                 **kwargs):\n",
    "        super(MultiHeadFCNHead, self).__init__()\n",
    "        assert num_convs >= 0\n",
    "        self.num_convs = num_convs\n",
    "        self.concat_input = concat_input\n",
    "        self.kernel_size = kernel_size\n",
    "        self._init_inputs(in_channels, in_index, input_transform)\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.in_index = in_index\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        if dropout_ratio > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_ratio)\n",
    "\n",
    "        conv_seg_head_list = []\n",
    "        for _ in range(self.num_head):\n",
    "            conv_seg_head_list.append(\n",
    "                nn.Conv2d(channels, num_classes, kernel_size=1))\n",
    "\n",
    "        self.conv_seg_head_list = nn.ModuleList(conv_seg_head_list)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        if num_convs == 0:\n",
    "            assert self.in_channels == self.channels\n",
    "\n",
    "        convs_list = []\n",
    "        conv_cat_list = []\n",
    "\n",
    "        for _ in range(self.num_head):\n",
    "            convs = []\n",
    "            convs.append(\n",
    "                ConvModule(\n",
    "                    self.in_channels,\n",
    "                    self.channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                    conv_cfg=self.conv_cfg,\n",
    "                    norm_cfg=self.norm_cfg,\n",
    "                    act_cfg=self.act_cfg))\n",
    "            for _ in range(num_convs - 1):\n",
    "                convs.append(\n",
    "                    ConvModule(\n",
    "                        self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "            if num_convs == 0:\n",
    "                convs_list.append(nn.Identity())\n",
    "            else:\n",
    "                convs_list.append(nn.Sequential(*convs))\n",
    "            if self.concat_input:\n",
    "                conv_cat_list.append(\n",
    "                    ConvModule(\n",
    "                        self.in_channels + self.channels,\n",
    "                        self.channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,\n",
    "                        conv_cfg=self.conv_cfg,\n",
    "                        norm_cfg=self.norm_cfg,\n",
    "                        act_cfg=self.act_cfg))\n",
    "\n",
    "        self.convs_list = nn.ModuleList(convs_list)\n",
    "        self.conv_cat_list = nn.ModuleList(conv_cat_list)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "\n",
    "        output_list = []\n",
    "        for head_idx in range(self.num_head):\n",
    "            output = self.convs_list[head_idx](x)\n",
    "            if self.concat_input:\n",
    "                output = self.conv_cat_list[head_idx](\n",
    "                    torch.cat([x, output], dim=1))\n",
    "            if self.dropout is not None:\n",
    "                output = self.dropout(output)\n",
    "            output = self.conv_seg_head_list[head_idx](output)\n",
    "            output_list.append(output)\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    def _init_inputs(self, in_channels, in_index, input_transform):\n",
    "        \"\"\"Check and initialize input transforms.\n",
    "\n",
    "        The in_channels, in_index and input_transform must match.\n",
    "        Specifically, when input_transform is None, only single feature map\n",
    "        will be selected. So in_channels and in_index must be of type int.\n",
    "        When input_transform\n",
    "\n",
    "        Args:\n",
    "            in_channels (int|Sequence[int]): Input channels.\n",
    "            in_index (int|Sequence[int]): Input feature index.\n",
    "            input_transform (str|None): Transformation type of input features.\n",
    "                Options: 'resize_concat', 'multiple_select', None.\n",
    "                'resize_concat': Multiple feature maps will be resize to the\n",
    "                    same size as first one and than concat together.\n",
    "                    Usually used in FCN head of HRNet.\n",
    "                'multiple_select': Multiple feature maps will be bundle into\n",
    "                    a list and passed into decode head.\n",
    "                None: Only one select feature map is allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if input_transform is not None:\n",
    "            assert input_transform in ['resize_concat', 'multiple_select']\n",
    "        self.input_transform = input_transform\n",
    "        self.in_index = in_index\n",
    "        if input_transform is not None:\n",
    "            assert isinstance(in_channels, (list, tuple))\n",
    "            assert isinstance(in_index, (list, tuple))\n",
    "            assert len(in_channels) == len(in_index)\n",
    "            if input_transform == 'resize_concat':\n",
    "                self.in_channels = sum(in_channels)\n",
    "            else:\n",
    "                self.in_channels = in_channels\n",
    "        else:\n",
    "            assert isinstance(in_channels, int)\n",
    "            assert isinstance(in_index, int)\n",
    "            self.in_channels = in_channels\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights of classification layer.\"\"\"\n",
    "        for conv_seg_head in self.conv_seg_head_list:\n",
    "            # normal_init(conv_seg_head, mean=0, std=0.01) don't have package\n",
    "            init.normal_(conv_seg_head.weight, mean=0, std=0.01)\n",
    "            if conv_seg_head.bias is not None:\n",
    "                init.constant_(conv_seg_head.bias, 0)\n",
    "\n",
    "    def _transform_inputs(self, inputs):\n",
    "        \"\"\"Transform inputs for decoder.\n",
    "\n",
    "        Args:\n",
    "            inputs (list[Tensor]): List of multi-level img features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The transformed inputs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_transform == 'resize_concat':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "            upsampled_inputs = [\n",
    "                F.interpolate(\n",
    "                    x,\n",
    "                    size=inputs[0].shape[2:],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners\n",
    "                    ) for x in inputs\n",
    "            ]\n",
    "            inputs = torch.cat(upsampled_inputs, dim=1)\n",
    "        elif self.input_transform == 'multiple_select':\n",
    "            inputs = [inputs[i] for i in self.in_index]\n",
    "        else:\n",
    "            inputs = inputs[self.in_index]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELRM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('base')\n",
    "\n",
    "\n",
    "class ERLM():\n",
    "    \"\"\"Editing Region Generation model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.device = torch.device('cpu') #cuda\n",
    "        self.is_train = opt['is_train']\n",
    "\n",
    "        clip_model, _ = clip.load('ViT-B/32', device=torch.device(\"cpu\"))\n",
    "        self.clip = clip_model.to(self.device)\n",
    "        self.encoder = AttrUNet(\n",
    "            in_channels=opt['encoder_in_channels'], attr_embedding=opt['text_embedding_dim']).to(self.device)\n",
    "        self.decoder = FCNHead(\n",
    "            in_channels=opt['fc_in_channels'],\n",
    "            in_index=opt['fc_in_index'],\n",
    "            channels=opt['fc_channels'],\n",
    "            num_convs=opt['fc_num_convs'],\n",
    "            concat_input=opt['fc_concat_input'],\n",
    "            dropout_ratio=opt['fc_dropout_ratio'],\n",
    "            num_classes=opt['fc_num_classes'],\n",
    "            align_corners=opt['fc_align_corners'],\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.init_training_settings()\n",
    "        self.palette = [[0, 0, 0], [255, 255, 255]]\n",
    "\n",
    "    def init_training_settings(self):\n",
    "        optim_params = []\n",
    "\n",
    "        for v in self.encoder.parameters():\n",
    "            if v.requires_grad:\n",
    "                optim_params.append(v)\n",
    "        for v in self.decoder.parameters():\n",
    "            if v.requires_grad:\n",
    "                optim_params.append(v)\n",
    "        # set up optimizers\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            optim_params,\n",
    "            self.opt['lr'],\n",
    "            weight_decay=self.opt['weight_decay'])\n",
    "        self.log_dict = OrderedDict()\n",
    "        self.entropy_loss = CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def feed_data(self, data):\n",
    "        self.image = data['image'].to(self.device)\n",
    "        self.mask = data['mask'].to(self.device)\n",
    "        text = data['text']\n",
    "        text_inputs = torch.cat([clip.tokenize(text)]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.text = self.clip.encode_text(text_inputs)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        self.text_enc = self.encoder(self.image, self.text)\n",
    "        self.seg_logits = self.decoder(self.text_enc)\n",
    "\n",
    "        loss = self.entropy_loss(self.seg_logits, self.mask)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_dict['loss_total'] = loss\n",
    "\n",
    "    def inference(self, data_loader, save_dir):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        acc = 0\n",
    "        num = 0\n",
    "\n",
    "        for _, data in enumerate(data_loader):\n",
    "            image = data['image'].to(self.device)\n",
    "            text = data['text']\n",
    "            text_inputs = torch.cat([clip.tokenize(text)]).to(self.device)\n",
    "            mask = data['mask'].to(self.device)\n",
    "            img_name = data['img_name']\n",
    "\n",
    "            num += image.size(0)\n",
    "            with torch.no_grad():\n",
    "                text_embedding = self.clip.encode_text(text_inputs)\n",
    "                text_enc = self.encoder(image, text_embedding)\n",
    "                seg_logits = self.decoder(text_enc)\n",
    "            seg_pred = seg_logits.argmax(dim=1)\n",
    "            acc += accuracy(seg_logits, mask)\n",
    "            palette_label = self.palette_result(mask.cpu().numpy())\n",
    "            palette_pred = self.palette_result(seg_pred.cpu().numpy())\n",
    "            image_numpy = image[0].cpu().numpy().astype(np.uint8).transpose(1, 2, 0)\n",
    "            image_numpy = image_numpy[..., ::-1]\n",
    "            concat_result = np.concatenate(\n",
    "                (image_numpy, palette_pred, palette_label), axis=1)\n",
    "            img_name_base, img_name_ext = os.path.splitext(img_name[0])\n",
    "            mmcv.imwrite(concat_result, f'{save_dir}/{img_name_base}_{text[0]}{img_name_ext}')\n",
    "\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        return (acc / num).item()\n",
    "\n",
    "    def get_current_log(self):\n",
    "        return self.log_dict\n",
    "\n",
    "    def update_learning_rate(self, epoch):\n",
    "        \"\"\"Update learning rate.\n",
    "\n",
    "        Args:\n",
    "            current_iter (int): Current iteration.\n",
    "            warmup_iter (int): Warmup iter numbers. -1 for no warmup.\n",
    "                Default: -1.\n",
    "        \"\"\"\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if self.opt['lr_decay'] == 'step':\n",
    "            lr = self.opt['lr'] * (\n",
    "                self.opt['gamma']**(epoch // self.opt['step']))\n",
    "        elif self.opt['lr_decay'] == 'cos':\n",
    "            lr = self.opt['lr'] * (\n",
    "                1 + math.cos(math.pi * epoch / self.opt['num_epochs'])) / 2\n",
    "        elif self.opt['lr_decay'] == 'linear':\n",
    "            lr = self.opt['lr'] * (1 - epoch / self.opt['num_epochs'])\n",
    "        elif self.opt['lr_decay'] == 'linear2exp':\n",
    "            if epoch < self.opt['turning_point'] + 1:\n",
    "                # learning rate decay as 95%\n",
    "                # at the turning point (1 / 95% = 1.0526)\n",
    "                lr = self.opt['lr'] * (\n",
    "                    1 - epoch / int(self.opt['turning_point'] * 1.0526))\n",
    "            else:\n",
    "                lr *= self.opt['gamma']\n",
    "        elif self.opt['lr_decay'] == 'schedule':\n",
    "            if epoch in self.opt['schedule']:\n",
    "                lr *= self.opt['gamma']\n",
    "        else:\n",
    "            raise ValueError('Unknown lr mode {}'.format(self.opt['lr_decay']))\n",
    "        # set learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    def save_network(self, save_path):\n",
    "        \"\"\"Save networks.\n",
    "        \"\"\"\n",
    "\n",
    "        save_dict = {}\n",
    "        save_dict['encoder'] = self.encoder.state_dict()\n",
    "        save_dict['decoder'] = self.decoder.state_dict()\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "\n",
    "    def load_network(self):\n",
    "        #checkpoint = torch.load(self.opt['elrm_model_path'])  # get correct model path!!! pretrained_model_path\n",
    "\n",
    "        #self.encoder.load_state_dict(\n",
    "         #   checkpoint['encoder'], strict=True)\n",
    "        #self.encoder.eval()\n",
    "\n",
    "        #self.decoder.load_state_dict(\n",
    "         #   checkpoint['decoder'], strict=True)\n",
    "        #self.decoder.eval()\n",
    "        \"\"\"Load the network weights.\"\"\"\n",
    "        # Add map_location to ensure the checkpoint is loaded on the CPU\n",
    "        checkpoint = torch.load(self.opt['elrm_model_path'], map_location=torch.device('cpu'))\n",
    "\n",
    "        self.encoder.load_state_dict(\n",
    "            checkpoint['encoder'], strict=True)\n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.decoder.load_state_dict(\n",
    "            checkpoint['decoder'], strict=True)\n",
    "        self.decoder.eval()\n",
    "\n",
    "    def palette_result(self, result):\n",
    "        seg = result[0]\n",
    "        palette = np.array(self.palette)\n",
    "        assert palette.shape[1] == 3\n",
    "        assert len(palette.shape) == 2\n",
    "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "        for label, color in enumerate(palette):\n",
    "            color_seg[seg == label, :] = color\n",
    "        # convert to BGR\n",
    "        color_seg = color_seg[..., ::-1]\n",
    "        return color_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ELRM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of yaml file converted to dictionary\n",
    "options_dict = {\n",
    "    'name': 'region_gen',\n",
    "    'use_tb_logger': True,\n",
    "    'debug_path': False,\n",
    "    'set_CUDA_VISIBLE_DEVICES': True,\n",
    "    'gpu_ids': [0],\n",
    "    'is_train' : False,\n",
    "\n",
    "    # dataset configs\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    #change depending on where you have files stored\n",
    "    'mask_dir': '../DFMM-Spotlight/mask',\n",
    "    'train_img_dir': '../DFMM-Spotlight/train_images',\n",
    "    'test_img_dir': '../DFMM-Spotlight/test_images',\n",
    "    'train_ann_file': '../DFMM-Spotlight/mask_ann/train_ann_file.jsonl',\n",
    "    'test_ann_file': '../DFMM-Spotlight/mask_ann/test_ann_file.jsonl',\n",
    "    'downsample_factor': 2,\n",
    "\n",
    "    # model configs\n",
    "    'model_type': 'ERLM',\n",
    "    'text_embedding_dim': 512,\n",
    "    'encoder_in_channels': 3,\n",
    "    'fc_in_channels': 64,\n",
    "    'fc_in_index': 4,\n",
    "    'fc_channels': 64,\n",
    "    'fc_num_convs': 1,\n",
    "    'fc_concat_input': False,\n",
    "    'fc_dropout_ratio': 0.1,\n",
    "    'fc_num_classes': 2,\n",
    "    'fc_align_corners': False,\n",
    "\n",
    "    # training configs\n",
    "    'val_freq': 5,\n",
    "    'print_freq': 100,\n",
    "    'weight_decay': 0,\n",
    "    'manual_seed': 2023,\n",
    "    'num_epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'lr_decay': \"step\",\n",
    "    'gamma': 0.1,\n",
    "    'step': [50],\n",
    "\n",
    "    #text prompt\n",
    "    \"text_prompt\": \"lower long flowy skirt with flowers\", # we can change this to any text prompt we want\n",
    "    \n",
    "    #paths (change this to our model paths)\n",
    "    'elrm_model_path' : '../region_generation_epoch60.pth', \n",
    "    'styleswap_model_path' : '../texfit-model',\n",
    "    'output_path' : 'example_output.png',\n",
    "    'img_path': '../MEN-Denim-id_00000089-03_7_additional.png',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Register the models in a simulated module list\n",
    "# This mimics scanning a folder for *_model.py files\n",
    "model_registry = {\n",
    "    'ERLM': ERLM\n",
    "}\n",
    "\n",
    "# Step 4: Define the dynamic model creation function\n",
    "def create_model(opt):\n",
    "    \"\"\"\n",
    "    Dynamically create a model based on the configuration dictionary.\n",
    "    \n",
    "    Args:\n",
    "        opt (dict): A dictionary containing model configuration. \n",
    "                    Must include 'model_type' key.\n",
    "    \n",
    "    Returns:\n",
    "        model (object): An instance of the specified model class.\n",
    "    \"\"\"\n",
    "    model_type = opt['model_type']\n",
    "    \n",
    "    # Find the model class from the registry\n",
    "    model_cls = model_registry.get(model_type)\n",
    "    \n",
    "    if model_cls is None:\n",
    "        raise ValueError(f\"Model '{model_type}' not found in registry.\")\n",
    "    \n",
    "    # Instantiate the model with options\n",
    "    model = model_cls(opt)\n",
    "    \n",
    "    # Optional logging — for demonstration\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(f\"Model [{model.__class__.__name__}] is created.\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load image\n",
    "def load_image(image_path):\n",
    "    #set downsample factor\n",
    "    downsample_factor = 2\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        #open image\n",
    "        image = Image.open(f)\n",
    "        width, height = image.size\n",
    "        #donwsample image\n",
    "        width = width // downsample_factor\n",
    "        height = height // downsample_factor\n",
    "        #resize image\n",
    "        image = image.resize(size=(width, height), resample=Image.NEAREST)\n",
    "        #transpose array\n",
    "        image = np.array(image).transpose(2, 0, 1)\n",
    "    #return array\n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse from utils.options, needed to read the options\n",
    "\n",
    "def parse(options_dict, is_train=True):\n",
    "    \"\"\"Parse options from a dictionary instead of a YAML file.\n",
    "\n",
    "    Args:\n",
    "        options_dict (dict): Dictionary containing model options.\n",
    "        is_train (bool): Indicates whether in training mode. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed options.\n",
    "    \"\"\"\n",
    "    # create copy of input dictionary to avoid modifying the original\n",
    "    opt = options_dict.copy()\n",
    "\n",
    "    #get gpu list\n",
    "    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n",
    "    if opt.get('set_CUDA_VISIBLE_DEVICES', None):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n",
    "        print('export CUDA_VISIBLE_DEVICES=' + gpu_list, flush=True)\n",
    "    else:\n",
    "        print('gpu_list: ', gpu_list, flush=True)\n",
    "\n",
    "    opt['is_train'] = is_train\n",
    "\n",
    "    #save optional paths (only if user wants to save logs & models)\n",
    "    opt['path'] = {}\n",
    "    opt['path']['root'] = os.getcwd() #set root as current working directory\n",
    "\n",
    "    if is_train:\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['models'] = opt.get('models_path', os.path.join(opt['path']['root'], 'models'))\n",
    "        opt['path']['logs'] = opt.get('logs_path', os.path.join(opt['path']['root'], 'logs'))\n",
    "        opt['path']['visualization'] = opt.get('visualization_path', os.path.join(opt['path']['root'], 'visualization'))\n",
    "        \n",
    "        \n",
    "        # change some options for debug mode\n",
    "        #debug enabled = True\n",
    "        if opt.get('debug', False):\n",
    "            opt['val_freq'] = 1\n",
    "            opt['print_freq'] = 1\n",
    "            opt['save_checkpoint_freq'] = 1\n",
    "\n",
    "    #for test mode    \n",
    "    else:  # test\n",
    "        #check for path, set to root if not set\n",
    "        opt['path']['results'] = opt.get('results_path', os.path.join(opt['path']['root'], 'results'))\n",
    "        opt['path']['log'] = opt.get('log_path', os.path.join(opt['path']['root'], 'test_logs'))\n",
    "        opt['path']['visualization'] = opt.get('vis_path', os.path.join(opt['path']['root'], 'test_visualizations'))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "import importlib\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #parse arguments from dictionary\n",
    "    opt = parse(options_dict, is_train=True)\n",
    "    \n",
    "    \n",
    "    #load model cnbfiguration\n",
    "    model = create_model(opt) #create model from options (need create_model function from models module))\n",
    "    model.load_network()\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "\n",
    "    #load image\n",
    "    img = load_image(opt['img_path'])\n",
    "    #conver to tensor\n",
    "    img= torch.from_numpy(img).unsqueeze(dim=0).to(model.device)\n",
    "    \n",
    "    #load text inputs with clip to encode text\n",
    "    text_input = torch.cat([clip.tokenize(opt[\"text_prompt\"])]).to(model.device)\n",
    "\n",
    "    #pass image and text through encoder\n",
    "    with torch.no_grad():\n",
    "        # text embedding\n",
    "        text_embedding = model.clip.encode_text(text_input)\n",
    "        #encode text\n",
    "        text_encoding = model.encoder(img, text_embedding)\n",
    "        seg_logits = model.decoder(text_encoding)\n",
    "\n",
    "    #argmax to get segmentation map\n",
    "    seg_pred = seg_logits.argmax(dim=1).cpu().numpy()[0]\n",
    "    #convert to grayscale image\n",
    "    seg_img = Image.fromarray(np.uint8(seg_pred * 255))\n",
    "\n",
    "    img = Image.open(opt['img_path']).convert(\"RGB\").resize((256, 512))\n",
    "\n",
    "    # Load pipeline\n",
    "    #load stable diffusion inpaingting model (we can change this if we want)\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        opt['styleswap_model_path'], revision=\"fp16\",\n",
    "        torch_dtype=torch.float32, #16\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "        #move model to gpu\n",
    "    ).to(\"cpu\") #cuda\n",
    "\n",
    "    #generate image\n",
    "    generator = torch.Generator(\"cpu\").manual_seed(2023) #cuda\n",
    "    images = pipe(\n",
    "        height=512,\n",
    "        width=256,\n",
    "        prompt=[opt['text_prompt']],\n",
    "        image=img,\n",
    "        mask_image=seg_img,\n",
    "        num_inference_steps=50,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "    #blend generated image with original image\n",
    "    final_img = Image.composite(images[0], img, seg_img)\n",
    "    #save image\n",
    "    final_img.save(opt['output_path'])\n",
    "    print('Saved edited result to', opt['output_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=128, out_channels=64\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=256, out_channels=128\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=512, out_channels=256\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Building UpConvBlock with upsample_cfg: {'type': 'InterpConv'}\n",
      "Initializing InterpConv with in_channels=1024, out_channels=512\n",
      "Upsample layer built: InterpConv(\n",
      "  (interp_upsample): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "InterpConv forward called with input shape: torch.Size([1, 1024, 32, 16])\n",
      "InterpConv forward called with input shape: torch.Size([1, 512, 64, 32])\n",
      "InterpConv forward called with input shape: torch.Size([1, 256, 128, 64])\n",
      "InterpConv forward called with input shape: torch.Size([1, 128, 256, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ae2c660bd0464b8787a0881cfa5e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch ../texfit-model/unet: Error no file named diffusion_pytorch_model.safetensors found in directory ../texfit-model/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch ../texfit-model/vae: Error no file named diffusion_pytorch_model.safetensors found in directory ../texfit-model/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d881384664364a47b05b5f3e579752b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved edited result to example_output.png\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
